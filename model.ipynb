{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e1f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023/12/10 21:25:30 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: No module named 'smmap'\n",
      "Registered model 'bert_model' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'bert_model'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7fe76fb207c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow.onnx\n",
    "import onnx\n",
    "model = onnx.load(\"/home/eternal/model_repository/test/1/bert_model.onnx\")\n",
    "mlflow.onnx.log_model(model, \"triton\", registered_model_name=\"bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314a5c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda:1\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_test.txt\n",
      "torch.Size([8, 512, 768])\n",
      "tensor([[-2.7437e-02, -1.7836e-02,  2.5362e-02,  ..., -2.1677e-01,\n",
      "         -1.1898e+00,  8.5863e-01],\n",
      "        [ 3.0832e-01, -1.3598e-03,  4.6901e-01,  ...,  2.2190e-01,\n",
      "         -7.5549e-01,  1.7428e-01],\n",
      "        [-8.6232e-01, -3.6574e-01, -1.0805e+00,  ...,  1.0557e-01,\n",
      "         -8.4608e-01,  6.7986e-01],\n",
      "        ...,\n",
      "        [-1.4538e+00, -2.5899e-01, -3.1744e-01,  ..., -8.9406e-01,\n",
      "         -4.7537e-01, -1.7409e+00],\n",
      "        [ 2.0653e-01,  1.1609e-01,  8.6789e-01,  ...,  3.7847e-02,\n",
      "         -6.8914e-01,  3.7743e-01],\n",
      "        [ 2.8966e-02, -2.1650e-01,  6.8210e-01,  ...,  2.8236e-01,\n",
      "         -9.1253e-01,  3.7012e-01]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "torch.Size([8, 768])\n",
      "tensor([[-0.5917,  0.3845, -0.2070,  ..., -0.7211,  0.3315,  0.6622],\n",
      "        [-0.9547,  0.2482, -0.8561,  ...,  0.2023,  0.2198,  0.8801],\n",
      "        [ 0.2540, -0.2534, -0.9997,  ...,  0.4349,  0.8973,  0.0185],\n",
      "        ...,\n",
      "        [-0.4217,  0.1669, -0.9998,  ..., -0.1531,  0.6804, -0.9260],\n",
      "        [-0.9300,  0.2070, -0.8138,  ...,  0.1035,  0.1884,  0.8221],\n",
      "        [-0.9470,  0.1696, -0.9091,  ...,  0.0969,  0.3210,  0.2820]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "torch.Size([8, 2])\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 5.38427, training accuracy: 0.62200: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.47317. Saving Model!\n",
      "epoch 01, loss: 0.67303, acc: 0.62200, val_loss: 0.47317, val_accuracy: 0.79000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 3.13939, training accuracy: 0.83300: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.47317 to 0.39540. Saving Model!\n",
      "epoch 02, loss: 0.39242, acc: 0.83300, val_loss: 0.39540, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 2.15636, training accuracy: 0.89300: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 03, loss: 0.26955, acc: 0.89300, val_loss: 0.44931, val_accuracy: 0.83000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 1.39275, training accuracy: 0.93800: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 04, loss: 0.17409, acc: 0.93800, val_loss: 0.50024, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.70281, training accuracy: 0.97200: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 05, loss: 0.08785, acc: 0.97200, val_loss: 0.60927, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.48723, training accuracy: 0.98300: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 06, loss: 0.06090, acc: 0.98300, val_loss: 0.99785, val_accuracy: 0.77400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.23520, training accuracy: 0.99000: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07, loss: 0.02940, acc: 0.99000, val_loss: 0.79309, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.32457, training accuracy: 0.98700: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08, loss: 0.04057, acc: 0.98700, val_loss: 0.79737, val_accuracy: 0.82000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.58187, training accuracy: 0.97500: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09, loss: 0.07273, acc: 0.97500, val_loss: 0.79569, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.17992, training accuracy: 0.99600: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 0.02249, acc: 0.99600, val_loss: 0.71067, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03398, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 0.00425, acc: 1.00000, val_loss: 0.75809, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01349, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 0.00169, acc: 1.00000, val_loss: 0.81224, val_accuracy: 0.83400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04865, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 0.00608, acc: 0.99900, val_loss: 1.05394, val_accuracy: 0.80200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.23626, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss: 0.02953, acc: 0.99300, val_loss: 1.02483, val_accuracy: 0.80200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.12850, training accuracy: 0.99500: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss: 0.01606, acc: 0.99500, val_loss: 0.85889, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.29005, training accuracy: 0.99100: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss: 0.03626, acc: 0.99100, val_loss: 0.90644, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.17552, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss: 0.02194, acc: 0.99300, val_loss: 0.87125, val_accuracy: 0.83600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09534, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss: 0.01192, acc: 0.99800, val_loss: 0.86931, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.08109, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss: 0.01014, acc: 0.99900, val_loss: 0.87955, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00774, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, loss: 0.00097, acc: 1.00000, val_loss: 0.93867, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00495, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, loss: 0.00062, acc: 1.00000, val_loss: 0.96935, val_accuracy: 0.83000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00538, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, loss: 0.00067, acc: 1.00000, val_loss: 1.05303, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00626, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23, loss: 0.00078, acc: 1.00000, val_loss: 0.99005, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00259, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, loss: 0.00032, acc: 1.00000, val_loss: 1.02667, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00283, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25, loss: 0.00035, acc: 1.00000, val_loss: 1.07211, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00347, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26, loss: 0.00043, acc: 1.00000, val_loss: 1.17275, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00151, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, loss: 0.00019, acc: 1.00000, val_loss: 1.15075, val_accuracy: 0.83400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00124, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28, loss: 0.00015, acc: 1.00000, val_loss: 1.16839, val_accuracy: 0.83400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.21394, training accuracy: 0.99200: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29, loss: 0.02674, acc: 0.99200, val_loss: 0.98619, val_accuracy: 0.83000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.32641, training accuracy: 0.98800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30, loss: 0.04080, acc: 0.98800, val_loss: 0.91833, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.27284, training accuracy: 0.99000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31, loss: 0.03411, acc: 0.99000, val_loss: 0.94509, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.31055, training accuracy: 0.98500: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32, loss: 0.03882, acc: 0.98500, val_loss: 0.83607, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01833, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33, loss: 0.00229, acc: 0.99900, val_loss: 1.02137, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.17401, training accuracy: 0.99500: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, loss: 0.02175, acc: 0.99500, val_loss: 0.84459, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03533, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, loss: 0.00442, acc: 0.99800, val_loss: 0.97111, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00438, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, loss: 0.00055, acc: 1.00000, val_loss: 0.95659, val_accuracy: 0.84000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05155, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37, loss: 0.00644, acc: 0.99700, val_loss: 1.07570, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.12055, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38, loss: 0.01507, acc: 0.99700, val_loss: 1.12820, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00517, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39, loss: 0.00065, acc: 1.00000, val_loss: 1.22235, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01796, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, loss: 0.00224, acc: 0.99900, val_loss: 1.48320, val_accuracy: 0.79600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06905, training accuracy: 0.99600: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41, loss: 0.00863, acc: 0.99600, val_loss: 1.23141, val_accuracy: 0.82000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09842, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42, loss: 0.01230, acc: 0.99700, val_loss: 1.19783, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00294, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43, loss: 0.00037, acc: 1.00000, val_loss: 1.21576, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00166, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44, loss: 0.00021, acc: 1.00000, val_loss: 1.22800, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00093, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45, loss: 0.00012, acc: 1.00000, val_loss: 1.24363, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00085, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, loss: 0.00011, acc: 1.00000, val_loss: 1.26039, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00476, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47, loss: 0.00059, acc: 1.00000, val_loss: 1.28042, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.20434, training accuracy: 0.99400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48, loss: 0.02554, acc: 0.99400, val_loss: 1.12876, val_accuracy: 0.80800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05285, training accuracy: 0.99800: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, loss: 0.00661, acc: 0.99800, val_loss: 1.10307, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05839, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss: 0.00730, acc: 0.99700, val_loss: 1.26947, val_accuracy: 0.83000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02408, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51, loss: 0.00301, acc: 0.99900, val_loss: 1.29178, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00123, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52, loss: 0.00015, acc: 1.00000, val_loss: 1.31388, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09615, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53, loss: 0.01202, acc: 0.99900, val_loss: 1.13083, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00551, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, loss: 0.00069, acc: 1.00000, val_loss: 1.25016, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00119, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55, loss: 0.00015, acc: 1.00000, val_loss: 1.27022, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00104, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56, loss: 0.00013, acc: 1.00000, val_loss: 1.28588, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00564, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57, loss: 0.00071, acc: 1.00000, val_loss: 1.30034, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00102, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58, loss: 0.00013, acc: 1.00000, val_loss: 1.33502, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01250, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59, loss: 0.00156, acc: 0.99900, val_loss: 1.32926, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00061, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60, loss: 0.00008, acc: 1.00000, val_loss: 1.35011, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.30659, training accuracy: 0.99100: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61, loss: 0.03832, acc: 0.99100, val_loss: 0.93650, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00980, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62, loss: 0.00122, acc: 1.00000, val_loss: 1.11044, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00239, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63, loss: 0.00030, acc: 1.00000, val_loss: 1.16871, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00166, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64, loss: 0.00021, acc: 1.00000, val_loss: 1.19605, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00134, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65, loss: 0.00017, acc: 1.00000, val_loss: 1.23547, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00410, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66, loss: 0.00051, acc: 1.00000, val_loss: 1.27666, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.18129, training accuracy: 0.99500: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67, loss: 0.02266, acc: 0.99500, val_loss: 1.37676, val_accuracy: 0.80400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.19584, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68, loss: 0.02448, acc: 0.99300, val_loss: 1.13030, val_accuracy: 0.82400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07140, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69, loss: 0.00892, acc: 0.99700, val_loss: 1.03647, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02303, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70, loss: 0.00288, acc: 0.99900, val_loss: 1.24674, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.18502, training accuracy: 0.99600: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71, loss: 0.02313, acc: 0.99600, val_loss: 1.00276, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02813, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72, loss: 0.00352, acc: 0.99800, val_loss: 1.13943, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00340, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73, loss: 0.00043, acc: 1.00000, val_loss: 1.17613, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06184, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74, loss: 0.00773, acc: 0.99900, val_loss: 1.13287, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02026, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75, loss: 0.00253, acc: 0.99900, val_loss: 1.20951, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03974, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76, loss: 0.00497, acc: 0.99700, val_loss: 1.15622, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00114, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77, loss: 0.00014, acc: 1.00000, val_loss: 1.15473, val_accuracy: 0.83600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00889, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78, loss: 0.00111, acc: 0.99900, val_loss: 1.65716, val_accuracy: 0.77000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03462, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79, loss: 0.00433, acc: 0.99900, val_loss: 1.51141, val_accuracy: 0.80200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00047, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80, loss: 0.00006, acc: 1.00000, val_loss: 1.52230, val_accuracy: 0.80200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00059, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81, loss: 0.00007, acc: 1.00000, val_loss: 1.48483, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00031, training accuracy: 1.00000: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82, loss: 0.00004, acc: 1.00000, val_loss: 1.49437, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00061, training accuracy: 1.00000: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83, loss: 0.00008, acc: 1.00000, val_loss: 1.48772, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00030, training accuracy: 1.00000: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84, loss: 0.00004, acc: 1.00000, val_loss: 1.49719, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00024, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85, loss: 0.00003, acc: 1.00000, val_loss: 1.50659, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00025, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86, loss: 0.00003, acc: 1.00000, val_loss: 1.51440, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00021, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87, loss: 0.00003, acc: 1.00000, val_loss: 1.52046, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00018, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88, loss: 0.00002, acc: 1.00000, val_loss: 1.52816, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00041, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89, loss: 0.00005, acc: 1.00000, val_loss: 1.54461, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00019, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90, loss: 0.00002, acc: 1.00000, val_loss: 1.57656, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00023, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91, loss: 0.00003, acc: 1.00000, val_loss: 1.58206, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00015, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92, loss: 0.00002, acc: 1.00000, val_loss: 1.59137, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00013, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93, loss: 0.00002, acc: 1.00000, val_loss: 1.60002, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00067, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94, loss: 0.00008, acc: 1.00000, val_loss: 1.66048, val_accuracy: 0.80800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00013, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95, loss: 0.00002, acc: 1.00000, val_loss: 1.66577, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00012, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96, loss: 0.00002, acc: 1.00000, val_loss: 1.66811, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00012, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97, loss: 0.00001, acc: 1.00000, val_loss: 1.67462, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00020, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98, loss: 0.00002, acc: 1.00000, val_loss: 1.61529, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00014, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99, loss: 0.00002, acc: 1.00000, val_loss: 1.61631, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.40007, training accuracy: 0.99100: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss: 0.05001, acc: 0.99100, val_loss: 0.90668, val_accuracy: 0.81800\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4694/986501826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"추론 시간: {inference_time:.3f}초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 한글 자연어 처리 데이터셋\n",
    "from Korpora import Korpora\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from tqdm import tqdm  # Progress Bar 출력\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "import math\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "# 토크나이저 관련 경고 무시하기 위하여 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "\n",
    "# device 지정\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'사용 디바이스: {device}')\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "\n",
    "train = pd.read_csv('~/Korpora/nsmc/ratings_train.txt', sep='\\t')\n",
    "test = pd.read_csv('~/Korpora/nsmc/ratings_test.txt', sep='\\t')\n",
    "train['length'] = train['document'].apply(lambda x: len(str(x)))\n",
    "test['length'] = test['document'].apply(lambda x: len(str(x)))\n",
    "train = train.loc[train['length'] > 5]\n",
    "# 전체 데이터셋 크기가 커서 1000개의 문장을 샘플링 합니다.\n",
    "train = train.sample(1000)\n",
    "test = test.loc[test['length'] > 5]\n",
    "test = test.sample(500)\n",
    "\n",
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, dataframe, tokenizer_pretrained):\n",
    "        # sentence, label 컬럼으로 구성된 데이터프레임 전달\n",
    "        self.data = dataframe        \n",
    "        # Huggingface 토크나이저 생성\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['document']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # 토큰화 처리\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    "            add_special_tokens=True  # 스페셜 토큰 적용\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)           # 2D -> 1D\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0) # 2D -> 1D\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        # input_ids, attention_mask, token_type_ids 이렇게 3가지 요소를 반환하도록 합니다.\n",
    "        # input_ids: 토큰\n",
    "        # attention_mask: 실제 단어가 존재하면 1, 패딩이면 0 (패딩은 0이 아닐 수 있습니다)\n",
    "        # token_type_ids: 문장을 구분하는 id. 단일 문장인 경우에는 전부 0\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label)\n",
    "\n",
    "\n",
    "# 토크나이저 지정\n",
    "tokenizer_pretrained = CHECKPOINT_NAME\n",
    "# train, test 데이터셋 생성\n",
    "train_data = TokenDataset(train, tokenizer_pretrained)\n",
    "test_data = TokenDataset(test, tokenizer_pretrained)\n",
    "\n",
    "# DataLoader로 이전에 생성한 Dataset를 지정하여, batch 구성, shuffle, num_workers 등을 설정합니다.\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# 1개의 batch 꺼내기\n",
    "inputs, labels = next(iter(train_loader))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터셋을 device 설정\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels.to(device)\n",
    "\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['token_type_ids'].shape\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['token_type_ids'].shape\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "config\n",
    "\n",
    "labels\n",
    "\n",
    "# 모델 생성\n",
    "model_bert = BertModel.from_pretrained(CHECKPOINT_NAME).to(device)\n",
    "model_bert\n",
    "\n",
    "\n",
    "output = model_bert(**inputs)\n",
    "output.keys()\n",
    "output['last_hidden_state'].shape, output['pooler_output'].shape\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "print(last_hidden_state.shape)\n",
    "print(last_hidden_state[:, 0, :])\n",
    "\n",
    "\n",
    "pooler_output = output['pooler_output']\n",
    "print(pooler_output.shape)\n",
    "print(pooler_output)\n",
    "\n",
    "fc = nn.Linear(768, 2)\n",
    "\n",
    "fc.to(device)\n",
    "fc_output = fc(last_hidden_state[:, 0, :])\n",
    "print(fc_output.shape)\n",
    "print(fc_output.argmax(dim=1))\n",
    "\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        # 부모클래스 초기화\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        start = time.time()\n",
    "        # 사전학습 모델 지정\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        # dropout 설정\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 최종 출력층 정의\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 입력을 pre-trained bert model 로 대입\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 결과의 last_hidden_state 가져옴\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        # last_hidden_state[:, 0, :]는 [CLS] 토큰을 가져옴\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC 을 거쳐 최종 출력\n",
    "        x = self.fc(x)\n",
    "        end = time.time()\n",
    "           \n",
    "        return x\n",
    "\n",
    "    \n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 옵티마이저 정의: bert.paramters()와 learning_rate 설정\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    # 모델을 훈련모드로 설정합니다. training mode 일 때 Gradient 가 업데이트 됩니다. 반드시 train()으로 모드 변경을 해야 합니다.\n",
    "    model.train()\n",
    "    \n",
    "    # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "    \n",
    "    # 예쁘게 Progress Bar를 출력하면서 훈련 상태를 모니터링 하기 위하여 tqdm으로 래핑합니다.\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    \n",
    "    # mini-batch 학습을 시작합니다.\n",
    "    for idx, (inputs, labels) in enumerate(prograss_bar):\n",
    "        # inputs, label 데이터를 device 에 올립니다. (cuda:0 혹은 cpu)\n",
    "        inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 누적 Gradient를 초기화 합니다.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward Propagation을 진행하여 결과를 얻습니다.\n",
    "        output = model(**inputs)\n",
    "        \n",
    "        # 손실함수에 output, label 값을 대입하여 손실을 계산합니다.\n",
    "        loss = loss_fn(output, labels)\n",
    "        \n",
    "        # 오차역전파(Back Propagation)을 진행하여 미분 값을 계산합니다.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 계산된 Gradient를 업데이트 합니다.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "        # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "        _, pred = output.max(dim=1)\n",
    "        \n",
    "        # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "        # 합계는 corr 변수에 누적합니다.\n",
    "        corr += pred.eq(labels).sum().item()\n",
    "        counts += len(labels)\n",
    "        \n",
    "        # loss 값은 1개 배치의 평균 손실(loss) 입니다. img.size(0)은 배치사이즈(batch size) 입니다.\n",
    "        # loss 와 img.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "        # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        # 프로그레스바에 학습 상황 업데이트\n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "        \n",
    "    # 누적된 정답수를 전체 개수로 나누어 주면 정확도가 산출됩니다.\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "    \n",
    "    # 평균 손실(loss)과 정확도를 반환합니다.\n",
    "    # train_loss, train_acc\n",
    "    return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "\n",
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    # model.eval()은 모델을 평가모드로 설정을 바꾸어 줍니다. \n",
    "    # dropout과 같은 layer의 역할 변경을 위하여 evaluation 진행시 꼭 필요한 절차 입니다.\n",
    "    model.eval()\n",
    "    \n",
    "    # Gradient가 업데이트 되는 것을 방지 하기 위하여 반드시 필요합니다.\n",
    "    with torch.no_grad():\n",
    "        # loss와 accuracy 계산을 위한 임시 변수 입니다. 0으로 초기화합니다.\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        \n",
    "        # 배치별 evaluation을 진행합니다.\n",
    "        for inputs, labels in data_loader:\n",
    "            # inputs, label 데이터를 device 에 올립니다. (cuda:0 혹은 cpu)\n",
    "            inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 모델에 Forward Propagation을 하여 결과를 도출합니다.\n",
    "            output = model(**inputs)\n",
    "            \n",
    "            # output의 max(dim=1)은 max probability와 max index를 반환합니다.\n",
    "            # max probability는 무시하고, max index는 pred에 저장하여 label 값과 대조하여 정확도를 도출합니다.\n",
    "            _, pred = output.max(dim=1)\n",
    "            \n",
    "            # pred.eq(lbl).sum() 은 정확히 맞춘 label의 합계를 계산합니다. item()은 tensor에서 값을 추출합니다.\n",
    "            # 합계는 corr 변수에 누적합니다.\n",
    "            corr += torch.sum(pred.eq(labels)).item()\n",
    "            \n",
    "            # loss 값은 1개 배치의 평균 손실(loss) 입니다. img.size(0)은 배치사이즈(batch size) 입니다.\n",
    "            # loss 와 img.size(0)를 곱하면 1개 배치의 전체 loss가 계산됩니다.\n",
    "            # 이를 누적한 뒤 Epoch 종료시 전체 데이터셋의 개수로 나누어 평균 loss를 산출합니다.\n",
    "            running_loss += loss_fn(output, labels).item() * labels.size(0)\n",
    "        \n",
    "        # validation 정확도를 계산합니다.\n",
    "        # 누적한 정답숫자를 전체 데이터셋의 숫자로 나누어 최종 accuracy를 산출합니다.\n",
    "        acc = corr / len(data_loader.dataset)\n",
    "        \n",
    "        # 결과를 반환합니다.\n",
    "        # val_loss, val_acc\n",
    "        return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# checkpoint로 저장할 모델의 이름을 정의 합니다.\n",
    "model_name = 'bert-kor-base'\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "# Epoch 별 훈련 및 검증을 수행합니다.\n",
    "for epoch in range(num_epochs):\n",
    "    # Model Training\n",
    "    # 훈련 손실과 정확도를 반환 받습니다.\n",
    "    train_loss, train_acc = model_train(bert, train_loader, loss_fn, optimizer, device)\n",
    "\n",
    "    # 검증 손실과 검증 정확도를 반환 받습니다.\n",
    "    val_loss, val_acc = model_evaluate(bert, test_loader, loss_fn, device)   \n",
    "    \n",
    "    # val_loss 가 개선되었다면 min_loss를 갱신하고 model의 가중치(weights)를 저장합니다.\n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        #dummy_input = torch.randn(1, 50000)\n",
    "        #torch.onnx.export(bert, input_ids, attention_mask, token_type_ids,'model.onnx')\n",
    "        torch.save(bert.state_dict(), f'{model_name}.pth')\n",
    "    \n",
    "\n",
    " #   print(f\"{end - start:.5f} sec\")\n",
    "    # Epoch 별 결과를 출력합니다.\n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')\n",
    "bert.load_state_dict(torch.load(f'{model_name}.pth'))\n",
    "import time#-------------------------------------------------------------------------------------------------------------\n",
    "'''class CustomPredictor():\n",
    "   \n",
    "\n",
    "    def __init__(self, model, tokenizer, labels: dict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        # 토큰화 처리\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    " \n",
    "           add_special_tokens=True  # 스페셜 토큰 적용\n",
    "        )\n",
    "        tokens.to(device)\n",
    "        prediction = self.model(**tokens)\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        output = prediction.argmax(dim=1).item()\n",
    "        prob, result = prediction.max(dim=1)[0].item(), self.labels[output]\n",
    "        print(f'[{result}]\\n확률은: {prob*100:.3f}% 입니다.')\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "labels = {\n",
    "    0: '부정 리뷰 입니다.', \n",
    "    1: '긍정 리뷰 입니다.'\n",
    "}\n",
    "\n",
    "predictor = CustomPredictor(bert, tokenizer, labels)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def predict_sentence(predictor):\n",
    "    input_sentence = input('문장을 입력해 주세요: ')\n",
    "    predictor.predict(input_sentence)\n",
    "\n",
    "predict_sentence(predictor) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a15aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c876607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장을 입력해 주세요: 야\n",
      "[부정 리뷰 입니다.]\n",
      "확률은: 84.699% 입니다.\n",
      "추론 시간: 0.005초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class CustomPredictor():\n",
    "    def __init__(self, model, tokenizer, labels: dict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        # 토큰화 처리\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    "            add_special_tokens=True  # 스페셜 토큰 적용\n",
    "        )\n",
    "        tokens.to(device)\n",
    "\n",
    "        # 추론 시작 시간 기록\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 모델 추론 수행\n",
    "        prediction = self.model(**tokens)\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        output = prediction.argmax(dim=1).item()\n",
    "        \n",
    "        # 추론 종료 시간 기록\n",
    "        end_time = time.time()\n",
    "\n",
    "        # 추론에 걸린 시간 계산 (초 단위)\n",
    "        inference_time = end_time - start_time\n",
    "\n",
    "        prob, result = prediction.max(dim=1)[0].item(), self.labels[output]\n",
    "        print(f'[{result}]\\n확률은: {prob*100:.3f}% 입니다.')\n",
    "        print(f\"추론 시간: {inference_time:.3f}초\")\n",
    "\n",
    "# 이후의 코드 (tokenizer 및 labels 정의, predictor 인스턴스 생성 등)는 동일하게 유지합니다.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "labels = {\n",
    "    0: '부정 리뷰 입니다.', \n",
    "    1: '긍정 리뷰 입니다.'\n",
    "}\n",
    "\n",
    "predictor = CustomPredictor(bert, tokenizer, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_sentence(predictor):\n",
    "    input_sentence = input('문장을 입력해 주세요: ')\n",
    "    predictor.predict(input_sentence)\n",
    "\n",
    "predict_sentence(predictor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3304eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534d3e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: tensor([[-0.1577, -0.0783]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eternal/.local/lib/python3.10/site-packages/torch/onnx/utils.py:2082: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input input_ids\n",
      "  warnings.warn(\n",
      "/home/eternal/.local/lib/python3.10/site-packages/torch/onnx/utils.py:2082: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input attention_mask\n",
      "  warnings.warn(\n",
      "/home/eternal/.local/lib/python3.10/site-packages/torch/onnx/utils.py:2082: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input token_type_ids\n",
      "  warnings.warn(\n",
      "/home/eternal/.local/lib/python3.10/site-packages/torch/onnx/utils.py:2082: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input output\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input_ids : Long(*, 6, strides=[6, 1], requires_grad=0, device=cpu),\n",
      "      %attention_mask : Long(*, 6, strides=[6, 1], requires_grad=0, device=cpu),\n",
      "      %token_type_ids : Long(*, 6, strides=[6, 1], requires_grad=0, device=cpu),\n",
      "      %bert.embeddings.word_embeddings.weight : Float(42000, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %bert.embeddings.position_embeddings.weight : Float(512, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %bert.embeddings.token_type_embeddings.weight : Float(2, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %bert.embeddings.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.embeddings.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.0.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.1.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.2.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.3.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.4.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.5.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.6.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.7.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.8.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.9.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.10.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.attention.self.query.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.attention.self.key.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.attention.self.value.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.attention.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.attention.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.attention.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.intermediate.dense.bias : Float(3072, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.output.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.output.LayerNorm.weight : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.encoder.layer.11.output.LayerNorm.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %bert.pooler.dense.weight : Float(768, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %bert.pooler.dense.bias : Float(768, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc.weight : Float(2, 768, strides=[768, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(2, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::MatMul_1468 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1469 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1472 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1478 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1479 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1480 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1481 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1482 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1485 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1491 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1492 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1493 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1494 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1495 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1498 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1504 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1505 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1506 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1507 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1508 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1511 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1517 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1518 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1519 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1520 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1521 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1524 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1530 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1531 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1532 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1533 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1534 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1537 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1543 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1544 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1545 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1546 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1547 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1550 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1556 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1557 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1558 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1559 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1560 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1563 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1569 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1570 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1571 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1572 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1573 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1576 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1582 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1583 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1584 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1585 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1586 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1589 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1595 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1596 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1597 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1598 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1599 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1602 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1608 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1609 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1610 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1611 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1612 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1615 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1621 : Float(768, 768, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1622 : Float(768, 3072, strides=[1, 768], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_1623 : Float(3072, 768, strides=[1, 3072], requires_grad=0, device=cpu)):\n",
      "  %/bert/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/bert/Constant\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert\n",
      "  %/bert/Constant_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/Constant_1\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:927:0\n",
      "  %/bert/Unsqueeze_output_0 : Long(*, 1, 6, strides=[6, 6, 1], requires_grad=0, device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/Unsqueeze\"](%attention_mask, %/bert/Constant_1_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:927:0\n",
      "  %/bert/Constant_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={2}, onnx_name=\"/bert/Constant_2\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:927:0\n",
      "  %/bert/Unsqueeze_1_output_0 : Long(*, 1, 1, 6, strides=[6, 6, 6, 1], requires_grad=0, device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/Unsqueeze_1\"](%/bert/Unsqueeze_output_0, %/bert/Constant_2_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:927:0\n",
      "  %/bert/Cast_output_0 : Float(*, 1, 1, 6, strides=[6, 6, 6, 1], requires_grad=0, device=cpu) = onnx::Cast[to=1, onnx_name=\"/bert/Cast\"](%/bert/Unsqueeze_1_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:938:0\n",
      "  %/bert/Constant_3_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/Constant_3\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/torch/_tensor.py:909:0\n",
      "  %/bert/Sub_output_0 : Float(*, 1, 1, 6, strides=[6, 6, 6, 1], requires_grad=0, device=cpu) = onnx::Sub[onnx_name=\"/bert/Sub\"](%/bert/Constant_3_output_0, %/bert/Cast_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/torch/_tensor.py:909:0\n",
      "  %/bert/Constant_4_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={-3.40282e+38}, onnx_name=\"/bert/Constant_4\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:939:0\n",
      "  %/bert/Mul_output_0 : Float(*, 1, 1, 6, strides=[6, 6, 6, 1], requires_grad=0, device=cpu) = onnx::Mul[onnx_name=\"/bert/Mul\"](%/bert/Sub_output_0, %/bert/Constant_4_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert # /home/eternal/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:939:0\n",
      "  %/bert/embeddings/Shape_output_0 : Long(2, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/embeddings/Shape\"](%input_ids), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:211:0\n",
      "  %/bert/embeddings/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/embeddings/Constant\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:211:0\n",
      "  %/bert/embeddings/Gather_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/embeddings/Gather\"](%/bert/embeddings/Shape_output_0, %/bert/embeddings/Constant_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:211:0\n",
      "  %onnx::Slice_218 : Long(1, 512, strides=[512, 1], requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %/bert/embeddings/Constant_1_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/embeddings/Constant_1\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %/bert/embeddings/Constant_2_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={0}, onnx_name=\"/bert/embeddings/Constant_2\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %/bert/embeddings/Constant_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"/bert/embeddings/Constant_3\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %/bert/embeddings/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/embeddings/Unsqueeze\"](%/bert/embeddings/Gather_output_0, %/bert/embeddings/Constant_3_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %/bert/embeddings/Constant_4_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/embeddings/Constant_4\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %/bert/embeddings/Slice_output_0 : Long(1, *, strides=[512, 1], requires_grad=0, device=cpu) = onnx::Slice[onnx_name=\"/bert/embeddings/Slice\"](%onnx::Slice_218, %/bert/embeddings/Constant_2_output_0, %/bert/embeddings/Unsqueeze_output_0, %/bert/embeddings/Constant_1_output_0, %/bert/embeddings/Constant_4_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:218:0\n",
      "  %/bert/embeddings/word_embeddings/Gather_output_0 : Float(*, 6, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Gather[onnx_name=\"/bert/embeddings/word_embeddings/Gather\"](%bert.embeddings.word_embeddings.weight, %input_ids), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::word_embeddings # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/functional.py:2233:0\n",
      "  %/bert/embeddings/token_type_embeddings/Gather_output_0 : Float(*, 6, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Gather[onnx_name=\"/bert/embeddings/token_type_embeddings/Gather\"](%bert.embeddings.token_type_embeddings.weight, %token_type_ids), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::token_type_embeddings # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/functional.py:2233:0\n",
      "  %/bert/embeddings/Add_output_0 : Float(*, 6, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/bert/embeddings/Add\"](%/bert/embeddings/word_embeddings/Gather_output_0, %/bert/embeddings/token_type_embeddings/Gather_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:235:0\n",
      "  %/bert/embeddings/position_embeddings/Gather_output_0 : Float(1, *, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Gather[onnx_name=\"/bert/embeddings/position_embeddings/Gather\"](%bert.embeddings.position_embeddings.weight, %/bert/embeddings/Slice_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.sparse.Embedding::position_embeddings # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/functional.py:2233:0\n",
      "  %/bert/embeddings/Add_1_output_0 : Float(*, *, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/bert/embeddings/Add_1\"](%/bert/embeddings/Add_output_0, %/bert/embeddings/position_embeddings/Gather_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:238:0\n",
      "  %/bert/embeddings/LayerNorm/LayerNormalization_output_0 : Float(*, *, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::LayerNormalization[axis=-1, epsilon=9.9999999999999998e-13, onnx_name=\"/bert/embeddings/LayerNorm/LayerNormalization\"](%/bert/embeddings/Add_1_output_0, %bert.embeddings.LayerNorm.weight, %bert.embeddings.LayerNorm.bias), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEmbeddings::embeddings/torch.nn.modules.normalization.LayerNorm::LayerNorm # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/functional.py:2543:0\n",
      "  %/bert/encoder/layer.0/attention/self/query/MatMul_output_0 : Float(*, *, 768, device=cpu) = onnx::MatMul[onnx_name=\"/bert/encoder/layer.0/attention/self/query/MatMul\"](%/bert/embeddings/LayerNorm/LayerNormalization_output_0, %onnx::MatMul_1468), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/bert/encoder/layer.0/attention/self/query/Add_output_0 : Float(*, *, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/bert/encoder/layer.0/attention/self/query/Add\"](%bert.encoder.layer.0.attention.self.query.bias, %/bert/encoder/layer.0/attention/self/query/MatMul_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::query # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/bert/encoder/layer.0/attention/self/key/MatMul_output_0 : Float(*, *, 768, device=cpu) = onnx::MatMul[onnx_name=\"/bert/encoder/layer.0/attention/self/key/MatMul\"](%/bert/embeddings/LayerNorm/LayerNormalization_output_0, %onnx::MatMul_1469), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/bert/encoder/layer.0/attention/self/key/Add_output_0 : Float(*, *, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/bert/encoder/layer.0/attention/self/key/Add\"](%bert.encoder.layer.0.attention.self.key.bias, %/bert/encoder/layer.0/attention/self/key/MatMul_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::key # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/bert/encoder/layer.0/attention/self/Shape_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/encoder/layer.0/attention/self/Shape\"](%/bert/encoder/layer.0/attention/self/key/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Gather_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Gather\"](%/bert/encoder/layer.0/attention/self/Shape_output_0, %/bert/encoder/layer.0/attention/self/Constant_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Shape_1_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/encoder/layer.0/attention/self/Shape_1\"](%/bert/encoder/layer.0/attention/self/key/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_1_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_1\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Gather_1_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Gather_1\"](%/bert/encoder/layer.0/attention/self/Shape_1_output_0, %/bert/encoder/layer.0/attention/self/Constant_1_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %onnx::Unsqueeze_248 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/bert/encoder/layer.0/attention/self/Unsqueeze_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/encoder/layer.0/attention/self/Unsqueeze\"](%/bert/encoder/layer.0/attention/self/Gather_output_0, %onnx::Unsqueeze_248), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %onnx::Unsqueeze_250 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/bert/encoder/layer.0/attention/self/Unsqueeze_1_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/encoder/layer.0/attention/self/Unsqueeze_1\"](%/bert/encoder/layer.0/attention/self/Gather_1_output_0, %onnx::Unsqueeze_250), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_2_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={12}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_2\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_3_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={64}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_3\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Concat_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Concat\"](%/bert/encoder/layer.0/attention/self/Unsqueeze_output_0, %/bert/encoder/layer.0/attention/self/Unsqueeze_1_output_0, %/bert/encoder/layer.0/attention/self/Constant_2_output_0, %/bert/encoder/layer.0/attention/self/Constant_3_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273:0\n",
      "  %/bert/encoder/layer.0/attention/self/Reshape_output_0 : Float(*, *, 12, 64, strides=[4608, 768, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Reshape\"](%/bert/encoder/layer.0/attention/self/key/Add_output_0, %/bert/encoder/layer.0/attention/self/Concat_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273:0\n",
      "  %/bert/encoder/layer.0/attention/self/value/MatMul_output_0 : Float(*, *, 768, device=cpu) = onnx::MatMul[onnx_name=\"/bert/encoder/layer.0/attention/self/value/MatMul\"](%/bert/embeddings/LayerNorm/LayerNormalization_output_0, %onnx::MatMul_1472), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/bert/encoder/layer.0/attention/self/value/Add_output_0 : Float(*, *, 768, strides=[4608, 768, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/bert/encoder/layer.0/attention/self/value/Add\"](%bert.encoder.layer.0.attention.self.value.bias, %/bert/encoder/layer.0/attention/self/value/MatMul_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self/torch.nn.modules.linear.Linear::value # /home/eternal/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  %/bert/encoder/layer.0/attention/self/Shape_2_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/encoder/layer.0/attention/self/Shape_2\"](%/bert/encoder/layer.0/attention/self/value/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_4_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_4\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Gather_2_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Gather_2\"](%/bert/encoder/layer.0/attention/self/Shape_2_output_0, %/bert/encoder/layer.0/attention/self/Constant_4_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Shape_3_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/encoder/layer.0/attention/self/Shape_3\"](%/bert/encoder/layer.0/attention/self/value/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_5_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_5\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Gather_3_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Gather_3\"](%/bert/encoder/layer.0/attention/self/Shape_3_output_0, %/bert/encoder/layer.0/attention/self/Constant_5_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %onnx::Unsqueeze_267 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/bert/encoder/layer.0/attention/self/Unsqueeze_2_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/encoder/layer.0/attention/self/Unsqueeze_2\"](%/bert/encoder/layer.0/attention/self/Gather_2_output_0, %onnx::Unsqueeze_267), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %onnx::Unsqueeze_269 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/bert/encoder/layer.0/attention/self/Unsqueeze_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/encoder/layer.0/attention/self/Unsqueeze_3\"](%/bert/encoder/layer.0/attention/self/Gather_3_output_0, %onnx::Unsqueeze_269), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_6_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={12}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_6\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_7_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={64}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_7\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Concat_1_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Concat_1\"](%/bert/encoder/layer.0/attention/self/Unsqueeze_2_output_0, %/bert/encoder/layer.0/attention/self/Unsqueeze_3_output_0, %/bert/encoder/layer.0/attention/self/Constant_6_output_0, %/bert/encoder/layer.0/attention/self/Constant_7_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273:0\n",
      "  %/bert/encoder/layer.0/attention/self/Reshape_1_output_0 : Float(*, *, 12, 64, strides=[4608, 768, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Reshape_1\"](%/bert/encoder/layer.0/attention/self/value/Add_output_0, %/bert/encoder/layer.0/attention/self/Concat_1_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273:0\n",
      "  %/bert/encoder/layer.0/attention/self/Transpose_output_0 : Float(*, 12, *, 64, strides=[4608, 64, 768, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/bert/encoder/layer.0/attention/self/Transpose\"](%/bert/encoder/layer.0/attention/self/Reshape_1_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:274:0\n",
      "  %/bert/encoder/layer.0/attention/self/Shape_4_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/encoder/layer.0/attention/self/Shape_4\"](%/bert/encoder/layer.0/attention/self/query/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_8_output_0 : Long(device=cpu) = onnx::Constant[value={0}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_8\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Gather_4_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Gather_4\"](%/bert/encoder/layer.0/attention/self/Shape_4_output_0, %/bert/encoder/layer.0/attention/self/Constant_8_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Shape_5_output_0 : Long(3, strides=[1], device=cpu) = onnx::Shape[onnx_name=\"/bert/encoder/layer.0/attention/self/Shape_5\"](%/bert/encoder/layer.0/attention/self/query/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_9_output_0 : Long(device=cpu) = onnx::Constant[value={1}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_9\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %/bert/encoder/layer.0/attention/self/Gather_5_output_0 : Long(device=cpu) = onnx::Gather[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Gather_5\"](%/bert/encoder/layer.0/attention/self/Shape_5_output_0, %/bert/encoder/layer.0/attention/self/Constant_9_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:272:0\n",
      "  %onnx::Unsqueeze_284 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/bert/encoder/layer.0/attention/self/Unsqueeze_4_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/encoder/layer.0/attention/self/Unsqueeze_4\"](%/bert/encoder/layer.0/attention/self/Gather_4_output_0, %onnx::Unsqueeze_284), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %onnx::Unsqueeze_286 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}]()\n",
      "  %/bert/encoder/layer.0/attention/self/Unsqueeze_5_output_0 : Long(1, strides=[1], device=cpu) = onnx::Unsqueeze[onnx_name=\"/bert/encoder/layer.0/attention/self/Unsqueeze_5\"](%/bert/encoder/layer.0/attention/self/Gather_5_output_0, %onnx::Unsqueeze_286), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_10_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={12}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_10\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_11_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={64}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_11\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self\n",
      "  %/bert/encoder/layer.0/attention/self/Concat_2_output_0 : Long(4, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Concat_2\"](%/bert/encoder/layer.0/attention/self/Unsqueeze_4_output_0, %/bert/encoder/layer.0/attention/self/Unsqueeze_5_output_0, %/bert/encoder/layer.0/attention/self/Constant_10_output_0, %/bert/encoder/layer.0/attention/self/Constant_11_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273:0\n",
      "  %/bert/encoder/layer.0/attention/self/Reshape_2_output_0 : Float(*, *, 12, 64, strides=[4608, 768, 64, 1], requires_grad=1, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/bert/encoder/layer.0/attention/self/Reshape_2\"](%/bert/encoder/layer.0/attention/self/query/Add_output_0, %/bert/encoder/layer.0/attention/self/Concat_2_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:273:0\n",
      "  %/bert/encoder/layer.0/attention/self/Transpose_1_output_0 : Float(*, 12, *, 64, strides=[4608, 64, 768, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/bert/encoder/layer.0/attention/self/Transpose_1\"](%/bert/encoder/layer.0/attention/self/Reshape_2_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:274:0\n",
      "  %/bert/encoder/layer.0/attention/self/Transpose_2_output_0 : Float(*, 12, 64, *, strides=[4608, 64, 1, 768], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 3, 1], onnx_name=\"/bert/encoder/layer.0/attention/self/Transpose_2\"](%/bert/encoder/layer.0/attention/self/Reshape_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:325:0\n",
      "  %/bert/encoder/layer.0/attention/self/MatMul_output_0 : Float(*, 12, *, *, strides=[432, 36, 6, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/bert/encoder/layer.0/attention/self/MatMul\"](%/bert/encoder/layer.0/attention/self/Transpose_1_output_0, %/bert/encoder/layer.0/attention/self/Transpose_2_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:325:0\n",
      "  %/bert/encoder/layer.0/attention/self/Constant_12_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={8}, onnx_name=\"/bert/encoder/layer.0/attention/self/Constant_12\"](), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:349:0\n",
      "  %/bert/encoder/layer.0/attention/self/Div_output_0 : Float(*, 12, *, *, strides=[432, 36, 6, 1], requires_grad=1, device=cpu) = onnx::Div[onnx_name=\"/bert/encoder/layer.0/attention/self/Div\"](%/bert/encoder/layer.0/attention/self/MatMul_output_0, %/bert/encoder/layer.0/attention/self/Constant_12_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:349:0\n",
      "  %/bert/encoder/layer.0/attention/self/Add_output_0 : Float(*, 12, *, *, strides=[432, 36, 6, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/bert/encoder/layer.0/attention/self/Add\"](%/bert/encoder/layer.0/attention/self/Div_output_0, %/bert/Mul_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/transformers.models.bert.modeling_bert.BertEncoder::encoder/transformers.models.bert.modeling_bert.BertLayer::layer.0/transformers.models.bert.modeling_bert.BertAttention::attention/transformers.models.bert.modeling_bert.BertSelfAttention::self # /home/eternal/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:352:0\n",
      "  %/bert/encoder/layer.0/attention/self/Softmax_output_0 : Float(*, 12, *, *, strides=[432, 36, 6, 1], requires_grad=1, device=cpu) = onnx::Softmax[axis=-1, onnx_name=\"/bert/encoder/layer.0/attention/self/Softmax\"](%/bert/encoder/layer.0/attention/self/Add_output_0), scope: __main__.SentimentClassifier::/transformers.models.bert.modeling_bert.BertModel::bert/tran"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_285360/607476077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0monnx_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sentiment_classifier.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdummy_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m torch.onnx.export(model, dummy_input, onnx_path, verbose=True,\n\u001b[0m\u001b[1;32m     38\u001b[0m                   \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                   \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             )\n\u001b[1;32m   1674\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exported graph: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m             \u001b[0monnx_proto_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0;31m# The ONNX checker only works for ONNX graph. So if the operator_export_type is not ONNX,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, bert_pretrained, num_classes=2, dropout_rate=0.1):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = output['pooler_output']\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# 토크나이저 및 모델 초기화\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model = SentimentClassifier(\"kykim/bert-kor-base\")\n",
    "\n",
    "# 입력 예시\n",
    "input_text = \"영화 정말 좋아요!\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "# 모델 추론\n",
    "output = model(input_ids, attention_mask, token_type_ids)\n",
    "print(\"Model Output:\", output)\n",
    "\n",
    "\n",
    "# ONNX로 변환\n",
    "onnx_path = \"sentiment_classifier.onnx\"\n",
    "dummy_input = (input_ids, attention_mask, token_type_ids)\n",
    "torch.onnx.export(model, dummy_input, onnx_path, verbose=True,\n",
    "                  input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input_ids': [0], 'attention_mask': [0], 'token_type_ids': [0], 'output': [0]})\n",
    "#print(test)\n",
    "#print(train)\n",
    "print(input_ids)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc30a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 2)\n",
      "Output values: [[-0.35465354  0.894657  ]]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import torch\n",
    "\n",
    "# 예제로 사용한 ONNX 파일의 경로\n",
    "onnx_path = \"bert_model.onnx\"\n",
    "\n",
    "# ONNX 파일 로드\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# ONNX Runtime 세션 생성\n",
    "ort_session = onnxruntime.InferenceSession(onnx_path)\n",
    "\n",
    "# 입력 데이터 생성 (실제 데이터를 사용하도록 수정 필요)\n",
    "sample_input = {\n",
    "    'input_ids': torch.ones((1, 128), dtype=torch.long).numpy(),\n",
    "    'attention_mask': torch.ones((1, 128), dtype=torch.long).numpy(),\n",
    "    'token_type_ids': torch.ones((1, 128), dtype=torch.long).numpy(),\n",
    "}\n",
    "\n",
    "# ONNX Runtime을 사용하여 추론 실행\n",
    "ort_inputs = {ort_session.get_inputs()[i].name: sample_input[name] for i, name in enumerate(['input_ids', 'attention_mask', 'token_type_ids'])}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# 추론 결과 출력\n",
    "print(\"Output shape:\", ort_outputs[0].shape)\n",
    "print(\"Output values:\", ort_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32321b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10120b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35639c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda:1\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_test.txt\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.63 GiB of which 107.44 MiB is free. Process 17985 has 8.68 GiB memory in use. Process 99943 has 11.03 GiB memory in use. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 2.83 GiB is allocated by PyTorch, and 67.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_247477/2759120548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooler_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 )\n\u001b[1;32m    606\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 427\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_query\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelative_position_scores_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacty of 23.63 GiB of which 107.44 MiB is free. Process 17985 has 8.68 GiB memory in use. Process 99943 has 11.03 GiB memory in use. Including non-PyTorch memory, this process has 3.34 GiB memory in use. Of the allocated memory 2.83 GiB is allocated by PyTorch, and 67.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 한글 자연어 처리 데이터셋\n",
    "from Korpora import Korpora\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from tqdm import tqdm  # Progress Bar 출력\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "import math\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "# 토크나이저 관련 경고 무시하기 위하여 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "\n",
    "# device 지정\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'사용 디바이스: {device}')\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "\n",
    "train = pd.read_csv('~/Korpora/nsmc/ratings_train.txt', sep='\\t')\n",
    "test = pd.read_csv('~/Korpora/nsmc/ratings_test.txt', sep='\\t')\n",
    "train['length'] = train['document'].apply(lambda x: len(str(x)))\n",
    "test['length'] = test['document'].apply(lambda x: len(str(x)))\n",
    "train = train.loc[train['length'] > 5]\n",
    "# 전체 데이터셋 크기가 커서 1000개의 문장을 샘플링 합니다.\n",
    "train = train.sample(1000)\n",
    "test = test.loc[test['length'] > 5]\n",
    "test = test.sample(500)\n",
    "\n",
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, dataframe, tokenizer_pretrained):\n",
    "        # sentence, label 컬럼으로 구성된 데이터프레임 전달\n",
    "        self.data = dataframe        \n",
    "        # Huggingface 토크나이저 생성\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['document']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # 토큰화 처리\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    "            add_special_tokens=True  # 스페셜 토큰 적용\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)           # 2D -> 1D\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0) # 2D -> 1D\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        # input_ids, attention_mask, token_type_ids 이렇게 3가지 요소를 반환하도록 합니다.\n",
    "        # input_ids: 토큰\n",
    "        # attention_mask: 실제 단어가 존재하면 1, 패딩이면 0 (패딩은 0이 아닐 수 있습니다)\n",
    "        # token_type_ids: 문장을 구분하는 id. 단일 문장인 경우에는 전부 0\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label)\n",
    "\n",
    "\n",
    "# 토크나이저 지정\n",
    "tokenizer_pretrained = CHECKPOINT_NAME\n",
    "# train, test 데이터셋 생성\n",
    "train_data = TokenDataset(train, tokenizer_pretrained)\n",
    "test_data = TokenDataset(test, tokenizer_pretrained)\n",
    "\n",
    "# DataLoader로 이전에 생성한 Dataset를 지정하여, batch 구성, shuffle, num_workers 등을 설정합니다.\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# 1개의 batch 꺼내기\n",
    "inputs, labels = next(iter(train_loader))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터셋을 device 설정\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels.to(device)\n",
    "\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['token_type_ids'].shape\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['token_type_ids'].shape\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "config\n",
    "\n",
    "labels\n",
    "\n",
    "# 모델 생성\n",
    "model_bert = BertModel.from_pretrained(CHECKPOINT_NAME).to(device)\n",
    "model_bert\n",
    "\n",
    "\n",
    "output = model_bert(**inputs)\n",
    "output.keys()\n",
    "output['last_hidden_state'].shape, output['pooler_output'].shape\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "print(last_hidden_state.shape)\n",
    "print(last_hidden_state[:, 0, :])\n",
    "\n",
    "\n",
    "pooler_output = output['pooler_output']\n",
    "print(pooler_output.shape)\n",
    "print(pooler_output)\n",
    "\n",
    "fc = nn.Linear(768, 2)\n",
    "\n",
    "fc.to(device)\n",
    "fc_output = fc(last_hidden_state[:, 0, :])\n",
    "print(fc_output.shape)\n",
    "print(fc_output.argmax(dim=1))\n",
    "\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        # 부모클래스 초기화\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        start = time.time()\n",
    "        # 사전학습 모델 지정\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        # dropout 설정\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 최종 출력층 정의\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 입력을 pre-trained bert model 로 대입\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 결과의 last_hidden_state 가져옴\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        # last_hidden_state[:, 0, :]는 [CLS] 토큰을 가져옴\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC 을 거쳐 최종 출력\n",
    "        x = self.fc(x)\n",
    "        end = time.time()\n",
    "           \n",
    "        return x\n",
    "\n",
    "    \n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 옵티마이저 정의: bert.paramters()와 learning_rate 설정\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "import torch.onnx\n",
    "import onnx\n",
    "\n",
    "# 모델과 토크나이저 정의\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT_NAME)\n",
    "model = CustomBertModel(CHECKPOINT_NAME)\n",
    "\n",
    "# 예제 입력 데이터 생성 (배치 크기 1)\n",
    "example_input = {\n",
    "    'input_ids': torch.zeros(1, 512, dtype=torch.long),\n",
    "    'attention_mask': torch.ones(1, 512, dtype=torch.long),\n",
    "    'token_type_ids': torch.zeros(1, 512, dtype=torch.long)\n",
    "}\n",
    "\n",
    "# 모델을 evaluation 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 모델의 forward 함수에 대한 정보를 제공하여 ONNX로 변환\n",
    "onnx_filename = \"custom_bert_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (example_input['input_ids'], example_input['attention_mask'], example_input['token_type_ids']),\n",
    "    onnx_filename,\n",
    "    input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}, \"token_type_ids\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model = onnx.load(onnx_filename)\n",
    "\n",
    "# ONNX 모델의 구조 출력\n",
    "print(onnx_model)\n",
    "\n",
    "# 모델의 input shape 확인\n",
    "print(\"Input shape:\", onnx_model.graph.input[0].type.tensor_type.shape.dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "dummy_input = (\n",
    "    inputs['input_ids'],\n",
    "    inputs['attention_mask'],\n",
    "    inputs['token_type_ids']\n",
    ")\n",
    "onnx_path = \"custom_bert_models.onnx\"\n",
    "torch.onnx.export(bert, dummy_input, onnx_path, input_names=['input_ids', 'attention_mask', 'token_type_ids'])\n",
    "\n",
    "print(f\"ONNX 모델이 {onnx_path} 경로에 저장되었습니다.\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be947d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eternal/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:245: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "dummy_model_input = tokenizer(\"This is a sample\", return_tensors=\"pt\")\n",
    "\n",
    "# export\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple(dummy_model_input.values()),\n",
    "    f=\"torch-model.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=13, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f353eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 19:05:37.679194: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-18 19:05:37.700760: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 19:05:37.826914: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 19:05:37.826970: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 19:05:37.827777: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 19:05:37.894810: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 19:05:38.504073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/eternal/.local/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:245: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load model and tokenizer\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "feature = \"sequence-classification\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=tokenizer,\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"trfs-model.onnx\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aed1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_input='나아가 끝나는 모시깽이'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eternal/.local/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"bert_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "onnx_program = torch.onnx.dynamo_export(onnx_model, torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2181633a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onnx_program' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11291/2673842810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0monnx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_program\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_torch_inputs_to_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input length: {len(onnx_input)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sample input: {onnx_input}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onnx_program' is not defined"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./bert_model.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce05b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 모델이 custom_bert_model.onnx로 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "dummy_input_ids = torch.randint(0, 100, (batch_size, 512)).to(device)\n",
    "dummy_attention_mask = torch.randint(0, 2, (batch_size, 512)).to(device)\n",
    "dummy_token_type_ids = torch.randint(0, 2, (batch_size, 512)).to(device)\n",
    "\n",
    "# 모델 인스턴스를 생성하고 device로 옮깁니다.\n",
    "bert_model = CustomBertModel(CHECKPOINT_NAME).to(device)\n",
    "\n",
    "# 모델을 평가 모드로 변경\n",
    "bert_model.eval()\n",
    "\n",
    "# 입력값에 대한 예측을 수행\n",
    "with torch.no_grad():\n",
    "    onnx_output = bert_model(dummy_input_ids, dummy_attention_mask, dummy_token_type_ids)\n",
    "\n",
    "# ONNX로 변환할 때 사용할 입력값 정의\n",
    "onnx_input = (dummy_input_ids, dummy_attention_mask, dummy_token_type_ids)\n",
    "\n",
    "# ONNX로 모델을 변환\n",
    "onnx_path = \"custom_bert_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    bert_model,\n",
    "    onnx_input,\n",
    "    onnx_path,\n",
    "    input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "    output_names=['output'],\n",
    "    opset_version=11,\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'token_type_ids': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"ONNX 모델이 {onnx_path}로 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dba00d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onnx_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17985/1606223584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 저장된 ONNX 모델을 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ONNX Runtime으로 모델 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onnx_path' is not defined"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "# 저장된 ONNX 모델을 불러오기\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# ONNX Runtime으로 모델 실행\n",
    "ort_session = onnxruntime.InferenceSession(onnx_path)\n",
    "\n",
    "# 입력값 생성\n",
    "ort_inputs = {\n",
    "    'input_ids': dummy_input_ids.cpu().numpy(),\n",
    "    'attention_mask': dummy_attention_mask.cpu().numpy(),\n",
    "    'token_type_ids': dummy_token_type_ids.cpu().numpy()\n",
    "}\n",
    "\n",
    "# ONNX Runtime으로 모델 실행\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "print(\"ONNX Runtime으로 실행된 결과:\", ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40824aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Names: ['input_ids', 'attention_mask', 'token_type_ids']\n",
      "Output Names: ['output']\n",
      "Input Shapes: {'input_ids': [dim_param: \"batch_size\"\n",
      ", dim_value: 512\n",
      "], 'attention_mask': [dim_param: \"batch_size\"\n",
      ", dim_value: 512\n",
      "], 'token_type_ids': [dim_param: \"batch_size\"\n",
      ", dim_value: 512\n",
      "]}\n",
      "Output Shapes: {'output': [dim_param: \"batch_size\"\n",
      ", dim_value: 2\n",
      "]}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model_path = \"custom_bert_model.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# 입력 및 출력 이름 및 모양 확인\n",
    "input_names = [input.name for input in onnx_model.graph.input]\n",
    "output_names = [output.name for output in onnx_model.graph.output]\n",
    "input_shapes = {input.name: input.type.tensor_type.shape.dim for input in onnx_model.graph.input}\n",
    "output_shapes = {output.name: output.type.tensor_type.shape.dim for output in onnx_model.graph.output}\n",
    "\n",
    "print(\"Input Names:\", input_names)\n",
    "print(\"Output Names:\", output_names)\n",
    "print(\"Input Shapes:\", input_shapes)\n",
    "print(\"Output Shapes:\", output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1635ceba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17985/805636199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 입력 데이터 생성 (예시)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model_path = \"custom_bert_model.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# 입력 데이터 생성 (예시)\n",
    "input_ids = np.random.randint(0, 100, (batch_size, 512)).astype(np.int64)\n",
    "attention_mask = np.random.randint(0, 2, (batch_size, 512)).astype(np.int64)\n",
    "token_type_ids = np.random.randint(0, 2, (batch_size, 512)).astype(np.int64)\n",
    "\n",
    "# ONNX Runtime으로 추론 수행\n",
    "ort_inputs = {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# ONNX 결과 확인\n",
    "print(\"ONNX Runtime으로 실행된 결과:\", ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4307d71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17985/1410221432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 정확도 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0monnx_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_predictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ONNX Accuracy: {onnx_accuracy * 100:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "onnx_predictions = np.argmax(ort_outs[0], axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "onnx_accuracy = np.sum(onnx_predictions == labels.numpy()) / len(labels)\n",
    "print(f'ONNX Accuracy: {onnx_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb69d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "text = \"안녕하세요, 테스트 문장입니다.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4340ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime으로 실행된 결과: [array([[ 0.5840088, -0.3010565]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# 텍스트 입력\n",
    "text = \"안녕하세요, 테스트 문장입니다.\"\n",
    "max_length = 512  # BERT의 최대 입력 길이\n",
    "\n",
    "# BERT 토크나이저 로드\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# 텍스트를 토큰화하고 입력 형태로 변환\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "input_ids = np.array(tokens[\"input_ids\"]).astype(np.int64)\n",
    "attention_mask = np.array(tokens[\"attention_mask\"]).astype(np.int64)\n",
    "token_type_ids = np.array(tokens[\"token_type_ids\"]).astype(np.int64)\n",
    "\n",
    "# ONNX 모델 로드\n",
    "onnx_model_path = \"custom_bert_model.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# 입력 데이터의 차원을 모델이 기대하는 형태로 맞추기\n",
    "ort_inputs = {\n",
    "    'input_ids': input_ids.reshape(1, -1),\n",
    "    'attention_mask': attention_mask.reshape(1, -1),\n",
    "    'token_type_ids': token_type_ids.reshape(1, -1)\n",
    "}\n",
    "\n",
    "# ONNX Runtime으로 추론 수행\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# ONNX 결과 확인\n",
    "print(\"ONNX Runtime으로 실행된 결과:\", ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa038e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f90adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c286b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d0085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6312bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda:1\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_test.txt\n",
      "torch.Size([8, 512, 768])\n",
      "tensor([[ 0.3325, -0.2499,  0.3809,  ..., -0.2590, -0.4662,  0.4997],\n",
      "        [ 0.4747,  0.6856,  0.0410,  ..., -0.0618, -0.2728,  0.4974],\n",
      "        [ 0.3427, -0.2806,  0.3988,  ...,  0.1963,  0.0688,  0.3202],\n",
      "        ...,\n",
      "        [-1.2455, -1.5762, -0.8590,  ..., -0.5634, -0.9576, -0.1357],\n",
      "        [-1.0842, -0.6757, -0.7633,  ..., -0.8032, -0.2658, -0.0784],\n",
      "        [-0.7359, -0.7591, -0.7097,  ..., -0.4552, -0.9406, -0.3521]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "torch.Size([8, 768])\n",
      "tensor([[-0.8246,  0.4620, -0.8645,  ...,  0.6346,  0.0743,  0.8430],\n",
      "        [-0.8107, -0.0127, -0.8022,  ..., -0.7424,  0.0474,  0.7857],\n",
      "        [-0.7827,  0.1503, -0.9316,  ..., -0.4612,  0.1976,  0.6368],\n",
      "        ...,\n",
      "        [ 0.2518,  0.5941, -0.9981,  ..., -0.1013,  0.7062, -0.2174],\n",
      "        [ 0.2516,  0.2663, -0.9994,  ...,  0.9411,  0.7196, -0.6923],\n",
      "        [-0.5509, -0.1927, -0.9991,  ...,  0.9992,  0.8883, -0.5011]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "torch.Size([8, 2])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "last_hidden_state shape: torch.Size([8, 512, 768])\n",
      "Output shape: torch.Size([8, 2])\n",
      "input_ids shape: torch.Size([1, 5])\n",
      "attention_mask shape: torch.Size([1, 5])\n",
      "token_type_ids shape: torch.Size([1, 5])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50058/2514477554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# 모델을 ONNX 형식으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m torch.onnx.export(\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     (\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1597\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_trace_quant_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jit_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_named_param_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0mprev_autocast_cache_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m     trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m     outs = ONNXTracedModule(\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     )(*args, **kwargs)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         graph, out = torch._C._create_graph_by_tracing(\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0min_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 한글 자연어 처리 데이터셋\n",
    "from Korpora import Korpora\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from tqdm import tqdm  # Progress Bar 출력\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "import math\n",
    "import time\n",
    "import torch.onnx\n",
    "\n",
    "# 토크나이저 관련 경고 무시하기 위하여 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "\n",
    "# device 지정\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'사용 디바이스: {device}')\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "\n",
    "train = pd.read_csv('~/Korpora/nsmc/ratings_train.txt', sep='\\t')\n",
    "test = pd.read_csv('~/Korpora/nsmc/ratings_test.txt', sep='\\t')\n",
    "train['length'] = train['document'].apply(lambda x: len(str(x)))\n",
    "test['length'] = test['document'].apply(lambda x: len(str(x)))\n",
    "train = train.loc[train['length'] > 5]\n",
    "# 전체 데이터셋 크기가 커서 1000개의 문장을 샘플링 합니다.\n",
    "train = train.sample(1000)\n",
    "test = test.loc[test['length'] > 5]\n",
    "test = test.sample(500)\n",
    "\n",
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "  \n",
    "    def __init__(self, dataframe, tokenizer_pretrained):\n",
    "        # sentence, label 컬럼으로 구성된 데이터프레임 전달\n",
    "        self.data = dataframe        \n",
    "        # Huggingface 토크나이저 생성\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['document']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # 토큰화 처리\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    "            add_special_tokens=True  # 스페셜 토큰 적용\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)           # 2D -> 1D\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0) # 2D -> 1D\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        # input_ids, attention_mask, token_type_ids 이렇게 3가지 요소를 반환하도록 합니다.\n",
    "        # input_ids: 토큰\n",
    "        # attention_mask: 실제 단어가 존재하면 1, 패딩이면 0 (패딩은 0이 아닐 수 있습니다)\n",
    "        # token_type_ids: 문장을 구분하는 id. 단일 문장인 경우에는 전부 0\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label)\n",
    "\n",
    "\n",
    "# 토크나이저 지정\n",
    "tokenizer_pretrained = CHECKPOINT_NAME\n",
    "# train, test 데이터셋 생성\n",
    "train_data = TokenDataset(train, tokenizer_pretrained)\n",
    "test_data = TokenDataset(test, tokenizer_pretrained)\n",
    "\n",
    "# DataLoader로 이전에 생성한 Dataset를 지정하여, batch 구성, shuffle, num_workers 등을 설정합니다.\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# 1개의 batch 꺼내기\n",
    "inputs, labels = next(iter(train_loader))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터셋을 device 설정\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels.to(device)\n",
    "\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['token_type_ids'].shape\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "inputs.keys()\n",
    "\n",
    "inputs['input_ids'].shape, inputs['attention_mask'].shape, inputs['token_type_ids'].shape\n",
    "\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "config\n",
    "\n",
    "labels\n",
    "\n",
    "# 모델 생성\n",
    "model_bert = BertModel.from_pretrained(CHECKPOINT_NAME).to(device)\n",
    "model_bert\n",
    "\n",
    "\n",
    "output = model_bert(**inputs)\n",
    "output.keys()\n",
    "output['last_hidden_state'].shape, output['pooler_output'].shape\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "print(last_hidden_state.shape)\n",
    "print(last_hidden_state[:, 0, :])\n",
    "\n",
    "\n",
    "pooler_output = output['pooler_output']\n",
    "print(pooler_output.shape)\n",
    "print(pooler_output)\n",
    "\n",
    "fc = nn.Linear(768, 2)\n",
    "\n",
    "fc.to(device)\n",
    "fc_output = fc(last_hidden_state[:, 0, :])\n",
    "print(fc_output.shape)\n",
    "print(fc_output.argmax(dim=1))\n",
    "\n",
    "'''\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        # 부모클래스 초기화\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        start = time.time()\n",
    "        # 사전학습 모델 지정\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        # dropout 설정\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 최종 출력층 정의\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 입력을 pre-trained bert model 로 대입\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 결과의 last_hidden_state 가져옴\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        # last_hidden_state[:, 0, :]는 [CLS] 토큰을 가져옴\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC 을 거쳐 최종 출력\n",
    "        x = self.fc(x)\n",
    "        end = time.time()\n",
    "           \n",
    "        return x\n",
    "'''\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        # 부모클래스 초기화\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        # 사전학습 모델 지정\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        # dropout 설정\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 최종 출력층 정의\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 입력을 pre-trained bert model 로 대입\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 결과의 last_hidden_state 가져옴\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        print(\"last_hidden_state shape:\", last_hidden_state.shape)\n",
    "        # last_hidden_state[:, 0, :]는 [CLS] 토큰을 가져옴\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC 을 거쳐 최종 출력\n",
    "        x = self.fc(x)\n",
    "        print(\"Output shape:\", x.shape)\n",
    "           \n",
    "        return x\n",
    "\n",
    "# 모델 생성\n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "\n",
    "# 1개의 batch 꺼내기\n",
    "inputs, labels = next(iter(train_loader))\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels.to(device)\n",
    "\n",
    "# forward 수행\n",
    "outputs = bert(**inputs)\n",
    "\n",
    "\n",
    "\n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 옵티마이저 정의: bert.paramters()와 learning_rate 설정\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "\n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = bert(**inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 학습된 모델 저장\n",
    "torch.save(bert.state_dict(), 'fine_tuned_bert.pth')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "model_checkpoint = \"kykim/bert-kor-base\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "model = BertModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 예제 입력 데이터 생성\n",
    "example_input = {\n",
    "    'input_ids': torch.tensor([tokenizer.encode(\"예제 문장\", max_length=512, padding='longest')]),\n",
    "    'attention_mask': torch.tensor([tokenizer.encode(\"예제 문장\", max_length=512, padding='longest')]),\n",
    "    'token_type_ids': torch.tensor([tokenizer.encode(\"예제 문장\", max_length=512, padding='longest')]),\n",
    "}\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 모델을 ONNX 형식으로 변환\n",
    "onnx_path = \"model.onnx\"\n",
    "\n",
    "# 각 텐서의 차원 확인\n",
    "print(\"input_ids shape:\", example_input['input_ids'].shape)\n",
    "print(\"attention_mask shape:\", example_input['attention_mask'].shape)\n",
    "print(\"token_type_ids shape:\", example_input['token_type_ids'].shape)\n",
    "\n",
    "# 모델을 ONNX 형식으로 변환\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (\n",
    "        example_input['input_ids'],\n",
    "        example_input['attention_mask'],\n",
    "        example_input['token_type_ids'],\n",
    "    ),\n",
    "    onnx_path,\n",
    "    verbose=True,\n",
    "    input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "    output_names=['output']\n",
    ")\n",
    "\n",
    "print(f\"ONNX 모델이 {onnx_path}에 저장되었습니다.\")\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import torch.onnx\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "model_checkpoint = \"kykim/bert-kor-base\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "model = BertModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 예제 입력 데이터 생성\n",
    "input_ids = torch.tensor([tokenizer.encode(\"아 재미없다.\", max_length=128, padding='longest')])\n",
    "attention_mask = torch.tensor([tokenizer.encode(\"아 재미없다.\", max_length=128, padding='longest')])\n",
    "\n",
    "token_type_ids = torch.tensor([tokenizer.encode(\"아 재미없다.\", max_length=128, padding='longest')])\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 모델을 ONNX 형식으로 변환\n",
    "onnx_path = \"model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (input_ids, attention_mask, token_type_ids),\n",
    "    onnx_path,\n",
    "    verbose=True,\n",
    "    input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "    output_names=['last_hidden_state', 'pooler_output']\n",
    ")\n",
    "\n",
    "print(f\"ONNX 모델이 {onnx_path}에 저장되었습니다.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98b4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Assuming 'bert' is an instance of CustomBertModel\n",
    "dummy_input = (torch.ones(1, 512).long().to(device),  # input_ids\n",
    "               torch.ones(1, 512).long().to(device),  # attention_mask\n",
    "               torch.zeros(1, 512).long().to(device))  # token_type_ids\n",
    "\n",
    "onnx_path = \"custom_bert_model.onnx\"\n",
    "torch.onnx.export(bert, dummy_input, onnx_path, input_names=['input_ids', 'attention_mask', 'token_type_ids'], output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0599a6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -1: [1, -1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99943/3668886923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming 'bert' is an instance of CustomBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m dummy_input = (torch.ones(1, -1).long().to(device),  # input_ids\n\u001b[0m\u001b[1;32m      5\u001b[0m                \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                torch.zeros(1, -1).long().to(device))  # token_type_ids\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -1: [1, -1]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da152c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12bc3e86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deployer_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99943/4254579807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeployer_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deployer_lib'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved. \n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License. \n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, \n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License. \n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import deployer_lib\n",
    "# \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('.')\n",
    "from modeling import BertForQuestionAnswering, BertConfig\n",
    "from tokenization import BertTokenizer\n",
    "from run_squad import convert_examples_to_features, read_squad_examples\n",
    "\n",
    "\n",
    "def get_model_args(model_args):\n",
    "    ''' the arguments initialize_model will receive '''\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ## Required parameters by the model. \n",
    "    parser.add_argument(\"--checkpoint\", \n",
    "                        default=None, \n",
    "                        type=str, \n",
    "                        required=True, \n",
    "                        help=\"The checkpoint of the model. \")\n",
    "    parser.add_argument('--batch_size', \n",
    "                        default=8, \n",
    "                        type=int, \n",
    "                        help='Batch size for inference')\n",
    "    parser.add_argument(\"--bert_model\", default=\"bert-large-uncased\", type=str, \n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                             \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
    "                             \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
    "    parser.add_argument(\"--do_lower_case\", \n",
    "                        action='store_true', \n",
    "                        help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
    "    parser.add_argument('--vocab_file', \n",
    "                        type=str, default=None, required=True, \n",
    "                        help=\"Vocabulary mapping/file BERT was pretrainined on\")\n",
    "    parser.add_argument(\"--predict_file\", default=None, type=str, \n",
    "                        help=\"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\")\n",
    "    parser.add_argument('--version_2_with_negative', \n",
    "                        action='store_true', \n",
    "                        help='If true, the SQuAD examples contain some that do not have an answer.')\n",
    "    parser.add_argument(\"--max_seq_length\", default=384, type=int, \n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n",
    "                             \"longer than this will be truncated, and sequences shorter than this will be padded.\")\n",
    "    parser.add_argument(\"--doc_stride\", default=128, type=int, \n",
    "                        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\")\n",
    "    parser.add_argument(\"--max_query_length\", default=64, type=int, \n",
    "                        help=\"The maximum number of tokens for the question. Questions longer than this will \" \n",
    "                             \"be truncated to this length.\")\n",
    "    parser.add_argument(\"--config_file\", \n",
    "                        default=None, \n",
    "                        type=str, \n",
    "                        required=True, \n",
    "                        help=\"The BERT model config\")\n",
    "    parser.add_argument('--fp16',\n",
    "                        action='store_true',\n",
    "                        help=\"use mixed-precision\")\n",
    "    parser.add_argument('--nbatches', \n",
    "                        default=2, \n",
    "                        type=int, \n",
    "                        help='Number of batches in the inference dataloader. Default: 10. ')\n",
    "    return parser.parse_args(model_args)\n",
    "\n",
    "\n",
    "def initialize_model(args):\n",
    "    ''' return model, ready to trace '''\n",
    "    config = BertConfig.from_json_file(args.config_file)\n",
    "    if config.vocab_size % 8 != 0:\n",
    "        config.vocab_size += 8 - (config.vocab_size % 8)\n",
    "    model = BertForQuestionAnswering(config)\n",
    "    model.enable_apex(False)\n",
    "    state_dict = torch.load(args.checkpoint, map_location='cpu')[\"model\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataloader(args):\n",
    "    ''' return dataloader for inference '''\n",
    "    \n",
    "    # Preprocess input data\n",
    "    tokenizer = BertTokenizer(args.vocab_file, do_lower_case=args.do_lower_case, max_len=512) # for bert large\n",
    "    \n",
    "    cached_features_file = args.predict_file + '_{}_{}.bin'.format(args.max_seq_length, args.doc_stride)\n",
    "    try:\n",
    "        with open(cached_features_file, \"rb\") as reader:\n",
    "            eval_features = pickle.load(reader)\n",
    "    except:\n",
    "        eval_examples = read_squad_examples(\n",
    "            input_file=args.predict_file,\n",
    "            is_training=False,\n",
    "            version_2_with_negative=args.version_2_with_negative)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            examples=eval_examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            doc_stride=args.doc_stride,\n",
    "            max_query_length=args.max_query_length,\n",
    "            is_training=False)\n",
    "        with open(cached_features_file, \"wb\") as writer:\n",
    "            pickle.dump(eval_features, writer)\n",
    "    \n",
    "    data = []\n",
    "    for feature in eval_features:\n",
    "        input_ids = torch.tensor(feature.input_ids, dtype=torch.int64)\n",
    "        input_mask = torch.tensor(feature.input_mask, dtype=torch.int64)\n",
    "        segment_ids = torch.tensor(feature.segment_ids, dtype=torch.int64)\n",
    "        inp = (input_ids, segment_ids, input_mask)\n",
    "        data.append(inp)\n",
    "    \n",
    "    if args.nbatches > 0:\n",
    "        data = data[:args.nbatches*args.batch_size]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        data, \n",
    "        batch_size=args.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=1, \n",
    "        pin_memory=True)\n",
    "    \n",
    "    return test_loader\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # don't touch this!\n",
    "    deployer, model_argv = deployer_lib.create_deployer(sys.argv[1:]) # deployer and returns removed deployer arguments\n",
    "    \n",
    "    model_args = get_model_args(model_argv)\n",
    "    \n",
    "    model = initialize_model(model_args)\n",
    "    dataloader = get_dataloader(model_args)\n",
    "    \n",
    "    deployer.deploy(dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddca8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: 줄 1: python: 명령어를 찾을 수 없음\r\n"
     ]
    }
   ],
   "source": [
    "!python ./deployer/deployer.py \\\n",
    "    --{exportFormat} \\\n",
    "    --save-dir ./candidatemodels \\\n",
    "    --triton-model-name {modelName} \\\n",
    "    --triton-model-version 1 \\\n",
    "    --triton-max-batch-size 8 \\\n",
    "    --triton-dyn-batching-delay 0 \\\n",
    "    --triton-engine-count 1 \\\n",
    "    -- --checkpoint ./data/bert_qa.pt \\\n",
    "    --config_file ./bert_config.json \\\n",
    "    --vocab_file ./vocab \\\n",
    "    --predict_file ./squad/v1.1/dev-v1.1.json \\\n",
    "    --do_lower_case \\\n",
    "    --batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704086b",
   "metadata": {},
   "source": [
    "최근꺼(12/19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551980ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda:1\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_test.txt\n",
      "torch.Size([8, 512, 768])\n",
      "tensor([[-0.4486, -0.4922, -1.4848,  ...,  0.0186, -0.9543,  0.4623],\n",
      "        [-1.1832, -0.3421, -1.3950,  ..., -0.7518, -1.3525, -0.3648],\n",
      "        [-0.0799, -0.5606, -0.2549,  ..., -0.1381, -1.8444,  0.5966],\n",
      "        ...,\n",
      "        [-0.1895,  0.1619, -0.5456,  ..., -0.5965, -1.1777, -1.2403],\n",
      "        [ 0.1769,  0.0495,  0.4708,  ..., -0.4505, -0.2279, -0.2820],\n",
      "        [ 0.6249, -0.1909,  0.6105,  ..., -0.1902, -0.8179,  0.4294]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "torch.Size([8, 768])\n",
      "tensor([[ 0.3223, -0.1357, -0.9996,  ...,  0.8833,  0.7770, -0.3804],\n",
      "        [-0.0275,  0.1542, -0.9998,  ...,  0.0086,  0.6127, -0.5531],\n",
      "        [-0.2391,  0.2380, -0.9987,  ..., -0.9552,  0.8146,  0.8728],\n",
      "        ...,\n",
      "        [ 0.2416,  0.7309, -0.9998,  ...,  0.9562,  0.4787,  0.7733],\n",
      "        [-0.5379,  0.2327, -0.8886,  ..., -0.7227,  0.4792,  0.5408],\n",
      "        [-0.9110,  0.3949, -0.8926,  ...,  0.5772,  0.2324,  0.7342]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "torch.Size([8, 2])\n",
      "tensor([0, 0, 0, 1, 1, 0, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 5.42143, training accuracy: 0.62400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.41643. Saving Model!\n",
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 01, loss: 0.67768, acc: 0.62400, val_loss: 0.41643, val_accuracy: 0.83000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 3.10725, training accuracy: 0.83400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.41643 to 0.39110. Saving Model!\n",
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 02, loss: 0.38841, acc: 0.83400, val_loss: 0.39110, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 2.01556, training accuracy: 0.89900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from 0.39110 to 0.38884. Saving Model!\n",
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 03, loss: 0.25194, acc: 0.89900, val_loss: 0.38884, val_accuracy: 0.83400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 1.31012, training accuracy: 0.94100: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 04, loss: 0.16376, acc: 0.94100, val_loss: 0.47530, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.60680, training accuracy: 0.97000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 05, loss: 0.07585, acc: 0.97000, val_loss: 0.59666, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.31575, training accuracy: 0.98900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 06, loss: 0.03947, acc: 0.98900, val_loss: 0.60069, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.25961, training accuracy: 0.99100: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 07, loss: 0.03245, acc: 0.99100, val_loss: 0.61817, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.15512, training accuracy: 0.99400: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 08, loss: 0.01939, acc: 0.99400, val_loss: 0.68669, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.10602, training accuracy: 0.99600: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 09, loss: 0.01325, acc: 0.99600, val_loss: 0.71727, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.22371, training accuracy: 0.99000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 10, loss: 0.02796, acc: 0.99000, val_loss: 0.70054, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09662, training accuracy: 0.99400: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 11, loss: 0.01208, acc: 0.99400, val_loss: 0.72910, val_accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.11672, training accuracy: 0.99600: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 12, loss: 0.01459, acc: 0.99600, val_loss: 0.79955, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.15707, training accuracy: 0.99400: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 13, loss: 0.01963, acc: 0.99400, val_loss: 0.88659, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.16699, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 14, loss: 0.02087, acc: 0.99300, val_loss: 0.70575, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.11643, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 15, loss: 0.01455, acc: 0.99700, val_loss: 0.72296, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03028, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 16, loss: 0.00378, acc: 0.99900, val_loss: 0.80936, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05217, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 17, loss: 0.00652, acc: 0.99900, val_loss: 0.86125, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00847, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 18, loss: 0.00106, acc: 1.00000, val_loss: 0.89765, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00279, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 19, loss: 0.00035, acc: 1.00000, val_loss: 0.93017, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.24728, training accuracy: 0.99200: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 20, loss: 0.03091, acc: 0.99200, val_loss: 0.81902, val_accuracy: 0.84000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.31027, training accuracy: 0.99000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 21, loss: 0.03878, acc: 0.99000, val_loss: 0.78453, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.27814, training accuracy: 0.98700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 22, loss: 0.03477, acc: 0.98700, val_loss: 0.77707, val_accuracy: 0.84000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03541, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 23, loss: 0.00443, acc: 0.99900, val_loss: 0.85461, val_accuracy: 0.84800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.19855, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 24, loss: 0.02482, acc: 0.99300, val_loss: 0.78583, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06911, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 25, loss: 0.00864, acc: 0.99700, val_loss: 0.82949, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00654, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 26, loss: 0.00082, acc: 1.00000, val_loss: 0.87015, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02461, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 27, loss: 0.00308, acc: 0.99900, val_loss: 0.85153, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01387, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 28, loss: 0.00173, acc: 1.00000, val_loss: 0.92830, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06761, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 29, loss: 0.00845, acc: 0.99800, val_loss: 0.94652, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02851, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 30, loss: 0.00356, acc: 0.99900, val_loss: 1.28817, val_accuracy: 0.80800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.17619, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 31, loss: 0.02202, acc: 0.99300, val_loss: 0.85654, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06590, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 32, loss: 0.00824, acc: 0.99800, val_loss: 0.89633, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04635, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 33, loss: 0.00579, acc: 0.99800, val_loss: 0.75539, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00648, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 34, loss: 0.00081, acc: 1.00000, val_loss: 0.81968, val_accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00469, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 35, loss: 0.00059, acc: 1.00000, val_loss: 0.81466, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00255, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 36, loss: 0.00032, acc: 1.00000, val_loss: 0.86308, val_accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00270, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 37, loss: 0.00034, acc: 1.00000, val_loss: 0.88417, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00203, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 38, loss: 0.00025, acc: 1.00000, val_loss: 0.90663, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00104, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 39, loss: 0.00013, acc: 1.00000, val_loss: 0.92878, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00096, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 40, loss: 0.00012, acc: 1.00000, val_loss: 0.94910, val_accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00062, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 41, loss: 0.00008, acc: 1.00000, val_loss: 0.96497, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00177, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 42, loss: 0.00022, acc: 1.00000, val_loss: 1.04775, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00050, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 43, loss: 0.00006, acc: 1.00000, val_loss: 1.04998, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00072, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 44, loss: 0.00009, acc: 1.00000, val_loss: 1.03377, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00043, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 45, loss: 0.00005, acc: 1.00000, val_loss: 1.02749, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00037, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 46, loss: 0.00005, acc: 1.00000, val_loss: 1.03224, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00039, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 47, loss: 0.00005, acc: 1.00000, val_loss: 1.03419, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00029, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 48, loss: 0.00004, acc: 1.00000, val_loss: 1.04184, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00027, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 49, loss: 0.00003, acc: 1.00000, val_loss: 1.05030, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00027, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 50, loss: 0.00003, acc: 1.00000, val_loss: 1.05735, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00034, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 51, loss: 0.00004, acc: 1.00000, val_loss: 1.05972, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00022, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 52, loss: 0.00003, acc: 1.00000, val_loss: 1.06783, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00020, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 53, loss: 0.00002, acc: 1.00000, val_loss: 1.07612, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00019, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 54, loss: 0.00002, acc: 1.00000, val_loss: 1.08541, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00024, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 55, loss: 0.00003, acc: 1.00000, val_loss: 1.08649, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00020, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 56, loss: 0.00002, acc: 1.00000, val_loss: 1.10116, val_accuracy: 0.87200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00017, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 57, loss: 0.00002, acc: 1.00000, val_loss: 1.10832, val_accuracy: 0.87200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00017, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 58, loss: 0.00002, acc: 1.00000, val_loss: 1.12764, val_accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00016, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 59, loss: 0.00002, acc: 1.00000, val_loss: 1.13439, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00013, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 60, loss: 0.00002, acc: 1.00000, val_loss: 1.13787, val_accuracy: 0.86600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00011, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 61, loss: 0.00001, acc: 1.00000, val_loss: 1.14485, val_accuracy: 0.86600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00014, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 62, loss: 0.00002, acc: 1.00000, val_loss: 1.16131, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00011, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 63, loss: 0.00001, acc: 1.00000, val_loss: 1.16828, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00009, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 64, loss: 0.00001, acc: 1.00000, val_loss: 1.17365, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00009, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 65, loss: 0.00001, acc: 1.00000, val_loss: 1.17876, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00011, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 66, loss: 0.00001, acc: 1.00000, val_loss: 1.18729, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00011, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 67, loss: 0.00001, acc: 1.00000, val_loss: 1.19344, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 68, loss: 0.00001, acc: 1.00000, val_loss: 1.19903, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00008, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 69, loss: 0.00001, acc: 1.00000, val_loss: 1.20859, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 70, loss: 0.00001, acc: 1.00000, val_loss: 1.21594, val_accuracy: 0.86200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00007, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 71, loss: 0.00001, acc: 1.00000, val_loss: 1.22358, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.19798, training accuracy: 0.99700: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 72, loss: 0.02475, acc: 0.99700, val_loss: 1.07069, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.62915, training accuracy: 0.97900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 73, loss: 0.07864, acc: 0.97900, val_loss: 0.74860, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03054, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 74, loss: 0.00382, acc: 0.99900, val_loss: 0.87031, val_accuracy: 0.85600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07991, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 75, loss: 0.00999, acc: 0.99700, val_loss: 0.91151, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.13451, training accuracy: 0.99400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 76, loss: 0.01681, acc: 0.99400, val_loss: 0.89782, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00460, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 77, loss: 0.00058, acc: 1.00000, val_loss: 0.94078, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07962, training accuracy: 0.99800: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 78, loss: 0.00995, acc: 0.99800, val_loss: 0.81682, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.28456, training accuracy: 0.98800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 79, loss: 0.03557, acc: 0.98800, val_loss: 0.92119, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02336, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 80, loss: 0.00292, acc: 0.99900, val_loss: 0.99935, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.14474, training accuracy: 0.99400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 81, loss: 0.01809, acc: 0.99400, val_loss: 0.82869, val_accuracy: 0.86000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06453, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 82, loss: 0.00807, acc: 0.99800, val_loss: 0.87632, val_accuracy: 0.82800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01070, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 83, loss: 0.00134, acc: 1.00000, val_loss: 0.87692, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00168, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 84, loss: 0.00021, acc: 1.00000, val_loss: 0.90233, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00113, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 85, loss: 0.00014, acc: 1.00000, val_loss: 0.91722, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00089, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 86, loss: 0.00011, acc: 1.00000, val_loss: 0.93089, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00091, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 87, loss: 0.00011, acc: 1.00000, val_loss: 0.94558, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00096, training accuracy: 1.00000: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 88, loss: 0.00012, acc: 1.00000, val_loss: 0.95745, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00066, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 89, loss: 0.00008, acc: 1.00000, val_loss: 0.96966, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00133, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 90, loss: 0.00017, acc: 1.00000, val_loss: 0.98085, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00056, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 91, loss: 0.00007, acc: 1.00000, val_loss: 0.99016, val_accuracy: 0.86600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00047, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 92, loss: 0.00006, acc: 1.00000, val_loss: 1.00297, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00038, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 93, loss: 0.00005, acc: 1.00000, val_loss: 1.01237, val_accuracy: 0.86800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00039, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 94, loss: 0.00005, acc: 1.00000, val_loss: 1.02352, val_accuracy: 0.87000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00029, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 95, loss: 0.00004, acc: 1.00000, val_loss: 1.03126, val_accuracy: 0.87000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00028, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 96, loss: 0.00004, acc: 1.00000, val_loss: 1.03919, val_accuracy: 0.87000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00059, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 97, loss: 0.00007, acc: 1.00000, val_loss: 1.07522, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00022, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 98, loss: 0.00003, acc: 1.00000, val_loss: 1.08218, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00028, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 99, loss: 0.00004, acc: 1.00000, val_loss: 1.09268, val_accuracy: 0.86400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00032, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Model has been converted to ONNX\n",
      "epoch 100, loss: 0.00004, acc: 1.00000, val_loss: 1.09870, val_accuracy: 0.86600\n",
      "문장을 입력해 주세요: ㅁ0\n",
      "[부정 리뷰 입니다.]\n",
      "확률은: 52.214% 입니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# 모델을 ONNX로 변환\\ndummy_input = {\\n    \\'input_ids\\': torch.zeros(1, 512).long().to(device),\\n    \\'attention_mask\\': torch.ones(1, 512).long().to(device),\\n    \\'token_type_ids\\': torch.zeros(1, 512).long().to(device)\\n}\\nonnx_path = \"bert_kor_base.onnx\"\\ntorch.onnx.export(bert, (dummy_input[\\'input_ids\\'], dummy_input[\\'attention_mask\\'], dummy_input[\\'token_type_ids\\']), onnx_path, input_names=[\\'input_ids\\', \\'attention_mask\\', \\'token_type_ids\\'], output_names=[\\'output\\'], dynamic_axes={\\'input_ids\\': {0: \\'batch_size\\'}, \\'attention_mask\\': {0: \\'batch_size\\'}, \\'token_type_ids\\': {0: \\'batch_size\\'}})\\n\\n# Triton Server 설정 파일 생성\\ntriton_config = f\"\"\"\\nname: \"bert_kor_base\"\\nplatform: \"onnxruntime_onnx\"\\n\\nmax_batch_size: 1\\ninput [\\n  {{\\n    name: \"input_ids\"\\n    data_type: TYPE_INT64\\n    dims: [512]\\n  }},\\n  {{\\n    name: \"attention_mask\"\\n    data_type: TYPE_INT64\\n    dims: [512]\\n  }},\\n  {{\\n    name: \"token_type_ids\"\\n    data_type: TYPE_INT64\\n    dims: [512]\\n  }}\\n]\\noutput [\\n  {{\\n    name: \"output\"\\n    data_type: TYPE_FP32\\n    dims: [2]\\n  }}\\n]\\n\"\"\"\\n\\n# Triton Server 설정 파일 저장\\nwith open(\"bert_kor_base_config.pbtxt\", \"w\") as f:\\n    f.write(triton_config)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizerFast\n",
    "from Korpora import Korpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 한글 자연어 처리 데이터셋\n",
    "from Korpora import Korpora\n",
    "\n",
    "# 토크나이저 관련 경고 무시하기 위하여 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "\n",
    "# device 지정\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'사용 디바이스: {device}')\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('~/Korpora/nsmc/ratings_train.txt', sep='\\t')\n",
    "test = pd.read_csv('~/Korpora/nsmc/ratings_test.txt', sep='\\t')\n",
    "train['length'] = train['document'].apply(lambda x: len(str(x)))\n",
    "test['length'] = test['document'].apply(lambda x: len(str(x)))\n",
    "train = train.loc[train['length'] > 5]\n",
    "# 전체 데이터셋 크기가 커서 1000개의 문장을 샘플링 합니다.\n",
    "train = train.sample(1000)\n",
    "test = test.loc[test['length'] > 5]\n",
    "test = test.sample(500)\n",
    "\n",
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer_pretrained):\n",
    "        self.data = dataframe        \n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['document']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label)\n",
    "\n",
    "tokenizer_pretrained = CHECKPOINT_NAME\n",
    "train_data = TokenDataset(train, tokenizer_pretrained)\n",
    "test_data = TokenDataset(test, tokenizer_pretrained)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "inputs, labels = next(iter(train_loader))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels.to(device)\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "labels\n",
    "\n",
    "model_bert = BertModel.from_pretrained(CHECKPOINT_NAME).to(device)\n",
    "\n",
    "output = model_bert(**inputs)\n",
    "output.keys()\n",
    "output['last_hidden_state'].shape, output['pooler_output'].shape\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "print(last_hidden_state.shape)\n",
    "print(last_hidden_state[:, 0, :])\n",
    "\n",
    "pooler_output = output['pooler_output']\n",
    "print(pooler_output.shape)\n",
    "print(pooler_output)\n",
    "\n",
    "fc = nn.Linear(768, 2)\n",
    "\n",
    "fc.to(device)\n",
    "fc_output = fc(last_hidden_state[:, 0, :])\n",
    "print(fc_output.shape)\n",
    "print(fc_output.argmax(dim=1))\n",
    "\n",
    "import time\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(prograss_bar):\n",
    "        inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(**inputs)\n",
    "        \n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred = output.max(dim=1)\n",
    "        corr += pred.eq(labels).sum().item()\n",
    "        counts += len(labels)\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "        \n",
    "    acc = corr / len(data_loader.dataset)\n",
    "    return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(**inputs)\n",
    "            \n",
    "            _, pred = output.max(dim=1)\n",
    "            \n",
    "            corr += torch.sum(pred.eq(labels)).item()\n",
    "            \n",
    "            running_loss += loss_fn(output, labels).item() * labels.size(0)\n",
    "        \n",
    "        acc = corr / len(data_loader.dataset)\n",
    "        return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "import torch.onnx\n",
    "\n",
    "num_epochs = 100\n",
    "model_name = 'bert-kor-base'\n",
    "min_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = model_train(bert, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = model_evaluate(bert, test_loader, loss_fn, device)   \n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        torch.save(bert.state_dict(), f'{model_name}.pth')\n",
    "        torch.onnx.export(bert, (inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids']), 'bert-kor-base.onnx', verbose=True)\n",
    "\n",
    "    \n",
    "    print(\" \") \n",
    "    print('Model has been converted to ONNX')\n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')\n",
    "\n",
    "bert.load_state_dict(torch.load(f'{model_name}.pth'))\n",
    "\n",
    "class CustomPredictor():\n",
    "    def __init__(self, model, tokenizer, labels: dict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        tokens.to(device)\n",
    "        prediction = self.model(**tokens)\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        output = prediction.argmax(dim=1).item()\n",
    "        prob, result = prediction.max(dim=1)[0].item(), self.labels[output]\n",
    "        print(f'[{result}]\\n확률은: {prob*100:.3f}% 입니다.')\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "labels = {\n",
    "    0: '부정 리뷰 입니다.', \n",
    "    1: '긍정 리뷰 입니다.'\n",
    "}\n",
    "\n",
    "predictor = CustomPredictor(bert, tokenizer, labels)\n",
    "\n",
    "def predict_sentence(predictor):\n",
    "    input_sentence = input('문장을 입력해 주세요: ')\n",
    "    predictor.predict(input_sentence)\n",
    "\n",
    "predict_sentence(predictor)\n",
    "\n",
    "# 모델을 ONNX로 변환\n",
    "dummy_input = {\n",
    "    'input_ids': torch.zeros(1, 512).long().to(device),\n",
    "    'attention_mask': torch.ones(1, 512).long().to(device),\n",
    "    'token_type_ids': torch.zeros(1, 512).long().to(device)\n",
    "}\n",
    "onnx_path = \"bert_kor_base.onnx\"\n",
    "torch.onnx.export(bert, (dummy_input['input_ids'], dummy_input['attention_mask'], dummy_input['token_type_ids']), onnx_path, input_names=['input_ids', 'attention_mask', 'token_type_ids'], output_names=['output'], dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'token_type_ids': {0: 'batch_size'}})\n",
    "\n",
    "# Triton Server 설정 파일 생성\n",
    "triton_config = f\"\"\"\n",
    "name: \"bert_kor_base\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "\n",
    "max_batch_size: 1\n",
    "input [\n",
    "  {{\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [512]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [512]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"token_type_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [512]\n",
    "  }}\n",
    "]\n",
    "output [\n",
    "  {{\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [2]\n",
    "  }}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Triton Server 설정 파일 저장\n",
    "with open(\"bert_kor_base_config.pbtxt\", \"w\") as f:\n",
    "    f.write(triton_config)\n",
    "#이게 그나마 성공 적인 모시꺵이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee0cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50648a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637eea44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81137acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2fa432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# 예제 모델\n",
    "example_model = CustomBertModel(CHECKPOINT_NAME)\n",
    "example_model.load_state_dict(torch.load('bert-kor-base.pth'))\n",
    "example_model.eval()\n",
    "\n",
    "# 예제 입력 데이터 (가변적인 배치 크기를 처리하기 위해 None 사용)\n",
    "example_inputs = {\n",
    "    'input_ids': torch.zeros((8,512), dtype=torch.long),\n",
    "    'attention_mask': torch.ones((8,512), dtype=torch.long),\n",
    "    'token_type_ids': torch.zeros((8,512), dtype=torch.long)\n",
    "}\n",
    "\n",
    "# ONNX로 모델 변환\n",
    "torch.onnx.export(\n",
    "    example_model,\n",
    "    (example_inputs['input_ids'], example_inputs['attention_mask'], example_inputs['token_type_ids']),\n",
    "    'bert-kor-base.onnx',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12cfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf3fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622be61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18f071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "295122b6",
   "metadata": {},
   "source": [
    "onnx파일 제조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a0db31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda:1\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /home/eternal/Korpora/nsmc/ratings_test.txt\n",
      "torch.Size([8, 512, 768])\n",
      "tensor([[-0.8574,  0.6325, -0.3674,  ..., -0.6722, -0.1680, -0.4679],\n",
      "        [ 0.7685,  0.1585, -0.4596,  ..., -0.1153, -1.9827,  0.6414],\n",
      "        [-1.3786,  0.2422, -0.0347,  ..., -0.7781, -1.1107, -1.2499],\n",
      "        ...,\n",
      "        [ 0.2785, -0.3443, -0.4524,  ...,  0.2754, -0.5510,  1.1015],\n",
      "        [ 0.5707, -0.4658, -0.1118,  ...,  0.0840, -0.9914,  0.4169],\n",
      "        [ 0.2421,  0.0193,  0.1748,  ...,  0.0201, -0.5366,  0.3620]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "torch.Size([8, 768])\n",
      "tensor([[ 0.8371, -0.0576, -0.5917,  ..., -0.9983,  0.5678, -0.1447],\n",
      "        [-0.8526,  0.3408, -0.9837,  ..., -0.8283,  0.5286,  0.7619],\n",
      "        [ 0.8362, -0.0338, -0.9868,  ..., -0.8323,  0.4803, -0.7215],\n",
      "        ...,\n",
      "        [-0.9151, -0.0977, -0.9960,  ..., -0.1104,  0.1922,  0.7855],\n",
      "        [-0.7069,  0.1817, -0.6323,  ...,  0.6076, -0.3552,  0.8089],\n",
      "        [-0.9262,  0.0514, -0.7951,  ..., -0.6902,  0.5161,  0.8301]],\n",
      "       device='cuda:0', grad_fn=<TanhBackward0>)\n",
      "torch.Size([8, 2])\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 5.30132, training accuracy: 0.63500: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val_loss has been improved from inf to 0.42087. Saving Model!\n",
      " \n",
      "epoch 01, loss: 0.66266, acc: 0.63500, val_loss: 0.42087, val_accuracy: 0.80800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 3.43187, training accuracy: 0.81800: 100%|█| 125/125 [00:15<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 02, loss: 0.42898, acc: 0.81800, val_loss: 0.47835, val_accuracy: 0.77600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 2.25257, training accuracy: 0.89100: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 03, loss: 0.28157, acc: 0.89100, val_loss: 0.49327, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 1.41566, training accuracy: 0.93200: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 04, loss: 0.17696, acc: 0.93200, val_loss: 0.47934, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.74748, training accuracy: 0.97100: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 05, loss: 0.09343, acc: 0.97100, val_loss: 0.61675, val_accuracy: 0.80600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.33239, training accuracy: 0.99100: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 06, loss: 0.04155, acc: 0.99100, val_loss: 0.72303, val_accuracy: 0.80600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.35121, training accuracy: 0.98400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 07, loss: 0.04390, acc: 0.98400, val_loss: 0.86415, val_accuracy: 0.77800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.34487, training accuracy: 0.98400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 08, loss: 0.04311, acc: 0.98400, val_loss: 0.77510, val_accuracy: 0.82000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.20798, training accuracy: 0.99200: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 09, loss: 0.02600, acc: 0.99200, val_loss: 0.78203, val_accuracy: 0.83400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.19207, training accuracy: 0.99200: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 10, loss: 0.02401, acc: 0.99200, val_loss: 0.88990, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.33659, training accuracy: 0.98400: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 11, loss: 0.04207, acc: 0.98400, val_loss: 0.77231, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.15112, training accuracy: 0.99400: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 12, loss: 0.01889, acc: 0.99400, val_loss: 0.82954, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02694, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 13, loss: 0.00337, acc: 1.00000, val_loss: 0.85171, val_accuracy: 0.83200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01550, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 14, loss: 0.00194, acc: 1.00000, val_loss: 0.89057, val_accuracy: 0.83000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07533, training accuracy: 0.99800: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 15, loss: 0.00942, acc: 0.99800, val_loss: 1.11712, val_accuracy: 0.79800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.16482, training accuracy: 0.99300: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 16, loss: 0.02060, acc: 0.99300, val_loss: 1.03390, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04890, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 17, loss: 0.00611, acc: 0.99800, val_loss: 0.98281, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06346, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 18, loss: 0.00793, acc: 0.99800, val_loss: 1.12047, val_accuracy: 0.80400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.18382, training accuracy: 0.99300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 19, loss: 0.02298, acc: 0.99300, val_loss: 0.94594, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.03072, training accuracy: 0.99900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 20, loss: 0.00384, acc: 0.99900, val_loss: 0.93890, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00958, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 21, loss: 0.00120, acc: 1.00000, val_loss: 1.07609, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09167, training accuracy: 0.99500: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 22, loss: 0.01146, acc: 0.99500, val_loss: 1.02974, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.13356, training accuracy: 0.99600: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 23, loss: 0.01670, acc: 0.99600, val_loss: 0.90000, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01292, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 24, loss: 0.00161, acc: 1.00000, val_loss: 0.92202, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00321, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 25, loss: 0.00040, acc: 1.00000, val_loss: 0.96878, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04605, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 26, loss: 0.00576, acc: 0.99700, val_loss: 1.02385, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.08798, training accuracy: 0.99600: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 27, loss: 0.01100, acc: 0.99600, val_loss: 0.88432, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01009, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 28, loss: 0.00126, acc: 1.00000, val_loss: 0.95371, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00839, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 29, loss: 0.00105, acc: 1.00000, val_loss: 0.98282, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00182, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 30, loss: 0.00023, acc: 1.00000, val_loss: 1.01189, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00191, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 31, loss: 0.00024, acc: 1.00000, val_loss: 1.02623, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00210, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 32, loss: 0.00026, acc: 1.00000, val_loss: 1.07209, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00111, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 33, loss: 0.00014, acc: 1.00000, val_loss: 1.09025, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00430, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 34, loss: 0.00054, acc: 1.00000, val_loss: 1.09149, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00081, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 35, loss: 0.00010, acc: 1.00000, val_loss: 1.11368, val_accuracy: 0.84800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00117, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 36, loss: 0.00015, acc: 1.00000, val_loss: 1.13335, val_accuracy: 0.84800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00101, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 37, loss: 0.00013, acc: 1.00000, val_loss: 1.14311, val_accuracy: 0.85200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00064, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 38, loss: 0.00008, acc: 1.00000, val_loss: 1.16059, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00046, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 39, loss: 0.00006, acc: 1.00000, val_loss: 1.17406, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00035, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 40, loss: 0.00004, acc: 1.00000, val_loss: 1.18336, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.56735, training accuracy: 0.98500: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 41, loss: 0.07092, acc: 0.98500, val_loss: 0.89327, val_accuracy: 0.80800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.57711, training accuracy: 0.97900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 42, loss: 0.07214, acc: 0.97900, val_loss: 0.64814, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02631, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 43, loss: 0.00329, acc: 1.00000, val_loss: 0.87085, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01511, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 44, loss: 0.00189, acc: 1.00000, val_loss: 0.87784, val_accuracy: 0.84400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.14623, training accuracy: 0.99400: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 45, loss: 0.01828, acc: 0.99400, val_loss: 0.82290, val_accuracy: 0.85000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09509, training accuracy: 0.99600: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 46, loss: 0.01189, acc: 0.99600, val_loss: 0.94190, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.02867, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 47, loss: 0.00358, acc: 0.99800, val_loss: 0.91158, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00391, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 48, loss: 0.00049, acc: 1.00000, val_loss: 0.99146, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00298, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 49, loss: 0.00037, acc: 1.00000, val_loss: 0.98852, val_accuracy: 0.84600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00189, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 50, loss: 0.00024, acc: 1.00000, val_loss: 1.02673, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00130, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 51, loss: 0.00016, acc: 1.00000, val_loss: 1.05936, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00087, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 52, loss: 0.00011, acc: 1.00000, val_loss: 1.07751, val_accuracy: 0.84000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00080, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 53, loss: 0.00010, acc: 1.00000, val_loss: 1.08973, val_accuracy: 0.84000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00435, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 54, loss: 0.00054, acc: 1.00000, val_loss: 1.20448, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.53886, training accuracy: 0.98300: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 55, loss: 0.06736, acc: 0.98300, val_loss: 0.81447, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.06136, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 56, loss: 0.00767, acc: 0.99900, val_loss: 0.92233, val_accuracy: 0.83800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05837, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 57, loss: 0.00730, acc: 0.99700, val_loss: 1.14129, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.11357, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 58, loss: 0.01420, acc: 0.99700, val_loss: 0.96598, val_accuracy: 0.82200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00520, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 59, loss: 0.00065, acc: 1.00000, val_loss: 1.01621, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00477, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 60, loss: 0.00060, acc: 1.00000, val_loss: 1.10256, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00206, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 61, loss: 0.00026, acc: 1.00000, val_loss: 1.11757, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00824, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 62, loss: 0.00103, acc: 0.99900, val_loss: 1.07248, val_accuracy: 0.84200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.15768, training accuracy: 0.99600: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 63, loss: 0.01971, acc: 0.99600, val_loss: 0.95137, val_accuracy: 0.82000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.15578, training accuracy: 0.99500: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 64, loss: 0.01947, acc: 0.99500, val_loss: 1.04618, val_accuracy: 0.77000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.10605, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 65, loss: 0.01326, acc: 0.99700, val_loss: 1.07705, val_accuracy: 0.77400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01066, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 66, loss: 0.00133, acc: 1.00000, val_loss: 0.99056, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07437, training accuracy: 0.99600: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 67, loss: 0.00930, acc: 0.99600, val_loss: 1.01760, val_accuracy: 0.80400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04378, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 68, loss: 0.00547, acc: 0.99900, val_loss: 1.01082, val_accuracy: 0.82000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.07892, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 69, loss: 0.00986, acc: 0.99700, val_loss: 0.98103, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00810, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 70, loss: 0.00101, acc: 1.00000, val_loss: 1.09156, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01086, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 71, loss: 0.00136, acc: 1.00000, val_loss: 1.13956, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00103, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 72, loss: 0.00013, acc: 1.00000, val_loss: 1.14851, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00131, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 73, loss: 0.00016, acc: 1.00000, val_loss: 1.18674, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00060, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 74, loss: 0.00008, acc: 1.00000, val_loss: 1.20362, val_accuracy: 0.81200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00082, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 75, loss: 0.00010, acc: 1.00000, val_loss: 1.21278, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00046, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 76, loss: 0.00006, acc: 1.00000, val_loss: 1.22586, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00061, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 77, loss: 0.00008, acc: 1.00000, val_loss: 1.23834, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00031, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 78, loss: 0.00004, acc: 1.00000, val_loss: 1.25163, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00047, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 79, loss: 0.00006, acc: 1.00000, val_loss: 1.27708, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00039, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 80, loss: 0.00005, acc: 1.00000, val_loss: 1.28710, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00027, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 81, loss: 0.00003, acc: 1.00000, val_loss: 1.29699, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00024, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 82, loss: 0.00003, acc: 1.00000, val_loss: 1.30809, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00029, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 83, loss: 0.00004, acc: 1.00000, val_loss: 1.31685, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00029, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 84, loss: 0.00004, acc: 1.00000, val_loss: 1.32427, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00017, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 85, loss: 0.00002, acc: 1.00000, val_loss: 1.33496, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00022, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 86, loss: 0.00003, acc: 1.00000, val_loss: 1.35025, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00016, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 87, loss: 0.00002, acc: 1.00000, val_loss: 1.35711, val_accuracy: 0.81600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00013, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 88, loss: 0.00002, acc: 1.00000, val_loss: 1.36693, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00013, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 89, loss: 0.00002, acc: 1.00000, val_loss: 1.37694, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00015, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 90, loss: 0.00002, acc: 1.00000, val_loss: 1.38923, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00043, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 91, loss: 0.00005, acc: 1.00000, val_loss: 1.47255, val_accuracy: 0.81000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.09824, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 92, loss: 0.01228, acc: 0.99800, val_loss: 1.98655, val_accuracy: 0.77000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.40877, training accuracy: 0.98900: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 93, loss: 0.05110, acc: 0.98900, val_loss: 0.98572, val_accuracy: 0.81800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.19202, training accuracy: 0.99200: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 94, loss: 0.02400, acc: 0.99200, val_loss: 1.14098, val_accuracy: 0.79800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.05048, training accuracy: 0.99800: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 95, loss: 0.00631, acc: 0.99800, val_loss: 1.07861, val_accuracy: 0.81400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.04358, training accuracy: 0.99700: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 96, loss: 0.00545, acc: 0.99700, val_loss: 1.03008, val_accuracy: 0.80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.01350, training accuracy: 0.99900: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 97, loss: 0.00169, acc: 0.99900, val_loss: 1.38357, val_accuracy: 0.80000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00521, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 98, loss: 0.00065, acc: 1.00000, val_loss: 1.16575, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00135, training accuracy: 1.00000: 100%|█| 125/125 [00:14<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 99, loss: 0.00017, acc: 1.00000, val_loss: 1.16953, val_accuracy: 0.82600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.00128, training accuracy: 1.00000: 100%|█| 125/125 [00:13<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "epoch 100, loss: 0.00016, acc: 1.00000, val_loss: 1.17725, val_accuracy: 0.82800\n",
      "문장을 입력해 주세요: ㅇㄴㄹ\n",
      "[부정 리뷰 입니다.]\n",
      "확률은: 87.539% 입니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig\n",
    "from transformers import BertTokenizerFast\n",
    "from Korpora import Korpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 한글 자연어 처리 데이터셋\n",
    "from Korpora import Korpora\n",
    "\n",
    "# 토크나이저 관련 경고 무시하기 위하여 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "\n",
    "# device 지정\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'사용 디바이스: {device}')\n",
    "\n",
    "corpus = Korpora.load(\"nsmc\")\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('~/Korpora/nsmc/ratings_train.txt', sep='\\t')\n",
    "test = pd.read_csv('~/Korpora/nsmc/ratings_test.txt', sep='\\t')\n",
    "train['length'] = train['document'].apply(lambda x: len(str(x)))\n",
    "test['length'] = test['document'].apply(lambda x: len(str(x)))\n",
    "train = train.loc[train['length'] > 5]\n",
    "# 전체 데이터셋 크기가 커서 1000개의 문장을 샘플링 합니다.\n",
    "train = train.sample(1000)\n",
    "test = test.loc[test['length'] > 5]\n",
    "test = test.sample(500)\n",
    "\n",
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer_pretrained):\n",
    "        self.data = dataframe        \n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.iloc[idx]['document']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids,\n",
    "        }, torch.tensor(label)\n",
    "\n",
    "tokenizer_pretrained = CHECKPOINT_NAME\n",
    "train_data = TokenDataset(train, tokenizer_pretrained)\n",
    "test_data = TokenDataset(test, tokenizer_pretrained)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=8, shuffle=True, num_workers=8)\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "inputs, labels = next(iter(train_loader))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "labels.to(device)\n",
    "\n",
    "config = BertConfig.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "labels\n",
    "\n",
    "model_bert = BertModel.from_pretrained(CHECKPOINT_NAME).to(device)\n",
    "\n",
    "output = model_bert(**inputs)\n",
    "output.keys()\n",
    "output['last_hidden_state'].shape, output['pooler_output'].shape\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "print(last_hidden_state.shape)\n",
    "print(last_hidden_state[:, 0, :])\n",
    "\n",
    "pooler_output = output['pooler_output']\n",
    "print(pooler_output.shape)\n",
    "print(pooler_output)\n",
    "\n",
    "fc = nn.Linear(768, 2)\n",
    "\n",
    "fc.to(device)\n",
    "fc_output = fc(last_hidden_state[:, 0, :])\n",
    "print(fc_output.shape)\n",
    "print(fc_output.argmax(dim=1))\n",
    "\n",
    "import time\n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        self.fc = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "bert = CustomBertModel(CHECKPOINT_NAME)\n",
    "bert.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def model_train(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(prograss_bar):\n",
    "        inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(**inputs)\n",
    "        \n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, pred = output.max(dim=1)\n",
    "        corr += pred.eq(labels).sum().item()\n",
    "        counts += len(labels)\n",
    "        \n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "        \n",
    "    acc = corr / len(data_loader.dataset)\n",
    "    return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "def model_evaluate(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(**inputs)\n",
    "            \n",
    "            _, pred = output.max(dim=1)\n",
    "            \n",
    "            corr += torch.sum(pred.eq(labels)).item()\n",
    "            \n",
    "            running_loss += loss_fn(output, labels).item() * labels.size(0)\n",
    "        \n",
    "        acc = corr / len(data_loader.dataset)\n",
    "        return running_loss / len(data_loader.dataset), acc\n",
    "\n",
    "import torch.onnx\n",
    "\n",
    "num_epochs = 100\n",
    "model_name = 'bert-kor-base'\n",
    "min_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = model_train(bert, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = model_evaluate(bert, test_loader, loss_fn, device)   \n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        print(f'[INFO] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model!')\n",
    "        min_loss = val_loss\n",
    "        torch.save(bert.state_dict(), f'{model_name}.pth')\n",
    "        #torch.onnx.export(bert, (inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids']), 'bert-kor-base.onnx', verbose=True)\n",
    "        # 예제 입력 생성 (배치 크기 1로 가정)\n",
    "        example_input = {\n",
    "            'input_ids': torch.zeros([1, 512], dtype=torch.long).to(device),\n",
    "            'attention_mask': torch.zeros([1, 512], dtype=torch.long).to(device),\n",
    "            'token_type_ids': torch.zeros([1, 512], dtype=torch.long).to(device),\n",
    "             }\n",
    "\n",
    "# ONNX로 변환\n",
    "        onnx_path = 'bert_kor_base.onnx'\n",
    "        torch.onnx.export(\n",
    "              bert,\n",
    "              args=(example_input['input_ids'], example_input['attention_mask'], example_input['token_type_ids']),\n",
    "              f=onnx_path,\n",
    "              input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "              output_names=['output'],\n",
    "              dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'token_type_ids': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "              opset_version=11,  # ONNX 버전\n",
    "              do_constant_folding=True,  # 상수 접기\n",
    "              verbose=True,\n",
    "              )\n",
    "    print(\" \") \n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy: {val_acc:.5f}')\n",
    "\n",
    "bert.load_state_dict(torch.load(f'{model_name}.pth'))\n",
    "\n",
    "\n",
    "class CustomPredictor():\n",
    "    def __init__(self, model, tokenizer, labels: dict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        \n",
    "    def predict(self, sentence):\n",
    "        tokens = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        tokens.to(device)\n",
    "        prediction = self.model(**tokens)\n",
    "        prediction = F.softmax(prediction, dim=1)\n",
    "        output = prediction.argmax(dim=1).item()\n",
    "        prob, result = prediction.max(dim=1)[0].item(), self.labels[output]\n",
    "        print(f'[{result}]\\n확률은: {prob*100:.3f}% 입니다.')\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(CHECKPOINT_NAME)\n",
    "\n",
    "labels = {\n",
    "    0: '부정 리뷰 입니다.', \n",
    "    1: '긍정 리뷰 입니다.'\n",
    "}\n",
    "\n",
    "predictor = CustomPredictor(bert, tokenizer, labels)\n",
    "\n",
    "def predict_sentence(predictor):\n",
    "    input_sentence = input('문장을 입력해 주세요: ')\n",
    "    predictor.predict(input_sentence)\n",
    "\n",
    "predict_sentence(predictor)\n",
    "\n",
    "\n",
    "import torch.onnx\n",
    "\n",
    "# 모델을 evaluation mode로 설정\n",
    "#bert.eval()\n",
    "\n",
    "\n",
    "\n",
    "# Triton Server config.pbtxt 파일 작성\n",
    "config_path = 'config.pbtxt'\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "name: \"bert_kor_base\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "\n",
    "max_batch_size: 8\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, 512 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"attention_mask\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, 512 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"token_type_ids\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1, 512 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 2 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 1, 2, 4, 8 ]\n",
    "  max_queue_delay_microseconds: 1000\n",
    "  default_priority: 0\n",
    "}\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c82054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567e820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8347c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7086/2920245680.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m '''\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# input_ids, attention_mask, token_type_ids를 Long 타입으로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "# 모델 초기화 및 가중치 로드\n",
    "model = CustomBertModel(\"kykim/bert-kor-base\")\n",
    "model.load_state_dict(torch.load('bert-kor-base.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ONNX 파일로 내보내기 위한 더미 입력 생성\n",
    "# 이 부분은 모델의 실제 입력과 일치해야 합니다.\n",
    "# 예를 들어, 모델이 128 토큰 길이의 입력을 받는다고 가정하면:\n",
    "input_ids = torch.randint(0, 20000, (1, 128)) # 모델에 맞는 임의의 토큰 ID\n",
    "attention_mask = torch.ones(1, 128)           # 모든 토큰이 \"real\"임을 나타내는 마스크\n",
    "token_type_ids = torch.zeros(1, 128)          # 단일 문장 입력을 나타내는 토큰 타입 ID\n",
    "'''\n",
    "# input_ids, attention_mask, token_type_ids를 Long 타입으로 변환\n",
    "input_ids = input_ids.long() if input_ids.dtype != torch.long else input_ids\n",
    "attention_mask = attention_mask.long() if attention_mask.dtype != torch.long else attention_mask\n",
    "token_type_ids = token_type_ids.long() if token_type_ids.dtype != torch.long else token_type_ids\n",
    "'''\n",
    "\n",
    "# 모든 더미 입력을 GPU로 이동 (GPU를 사용하는 경우)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "# 모델과 더미 입력을 ONNX로 내보내기\n",
    "torch.onnx.export(model, \n",
    "                  (input_ids, attention_mask, token_type_ids), \n",
    "                  \"bert_kor_base.onnx\",\n",
    "                  export_params=True, \n",
    "                  opset_version=11, \n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'},\n",
    "                                'attention_mask': {0: 'batch_size'},\n",
    "                                'token_type_ids': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d27b4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "# 모델 초기화 및 가중치 로드\n",
    "model = CustomBertModel(\"kykim/bert-kor-base\")\n",
    "model.load_state_dict(torch.load('bert-kor-base.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ONNX 파일로 내보내기 위한 더미 입력 생성\n",
    "# 모델이 128 토큰 길이의 입력을 받는다고 가정\n",
    "input_ids = torch.randint(0, 20000, (1, 512)) # 모델에 맞는 임의의 토큰 ID\n",
    "attention_mask = torch.ones(1, 512)           # 모든 토큰이 \"real\"임을 나타내는 마스크\n",
    "token_type_ids = torch.zeros(1, 512)          # 단일 문장 입력을 나타내는 토큰 타입 ID\n",
    "\n",
    "# 모든 더미 입력을 Long 타입으로 변환하고 GPU로 이동 (GPU를 사용하는 경우)\n",
    "input_ids = input_ids.long().to(device)\n",
    "attention_mask = attention_mask.long().to(device)\n",
    "token_type_ids = token_type_ids.long().to(device)\n",
    "\n",
    "# 모델과 더미 입력을 ONNX로 내보내기\n",
    "torch.onnx.export(model, \n",
    "                  (input_ids, attention_mask, token_type_ids), \n",
    "                  \"bert_kor_base.onnx\",\n",
    "                  export_params=True, \n",
    "                  opset_version=11, \n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'},\n",
    "                                'attention_mask': {0: 'batch_size'},\n",
    "                                'token_type_ids': {0: 'batch_size'},\n",
    "                                'output': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ef083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.38527295,  1.0995104 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# ONNX 모델 로드\n",
    "session = ort.InferenceSession(\"bert_kor_base.onnx\")\n",
    "\n",
    "# 모델 입력을 위한 더미 데이터 생성\n",
    "# 실제 데이터와 동일한 형식을 사용하세요.\n",
    "input_ids = np.random.randint(0, 20000, size=(1, 128), dtype=np.int64)\n",
    "attention_mask = np.ones((1, 128), dtype=np.int64)\n",
    "token_type_ids = np.zeros((1, 128), dtype=np.int64)\n",
    "\n",
    "# 모델 실행\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_mask,\n",
    "    \"token_type_ids\": token_type_ids\n",
    "}\n",
    "outputs = session.run(None, inputs)\n",
    "\n",
    "# 출력 확인\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04cb7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15125568 0.84874432]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = np.array([-0.50374013, 1.2210462])\n",
    "probabilities = F.softmax(torch.tensor(logits), dim=0).numpy()\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347a979",
   "metadata": {},
   "source": [
    "클라이언트 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 서버 URL 설정\n",
    "TRITON_SERVER_URL = \"localhost:8000\"\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "\n",
    "# 클라이언트 객체 생성\n",
    "client = httpclient.InferenceServerClient(url=TRITON_SERVER_URL)\n",
    "\n",
    "# 분석할 텍스트\n",
    "#text = \"여기에 분석할 텍스트를 입력하세요\"\n",
    "# 분석할 텍스트 입력 받기\n",
    "text = input(\"분석할 텍스트를 입력하세요: \")\n",
    "\n",
    "\n",
    "# 토크나이징 및 텐서 변환\n",
    "inputs = tokenizer(text, return_tensors=\"np\", max_length=128, truncation=True, padding='max_length')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "# Triton 서버 추론 요청\n",
    "triton_inputs = [\n",
    "    httpclient.InferInput(\"input_ids\", input_ids.shape, \"INT64\"),\n",
    "    httpclient.InferInput(\"attention_mask\", attention_mask.shape, \"INT64\"),\n",
    "    httpclient.InferInput(\"token_type_ids\", token_type_ids.shape, \"INT64\"),\n",
    "]\n",
    "\n",
    "triton_inputs[0].set_data_from_numpy(input_ids)\n",
    "triton_inputs[1].set_data_from_numpy(attention_mask)\n",
    "triton_inputs[2].set_data_from_numpy(token_type_ids)\n",
    "\n",
    "response = client.infer(\"bert_kor_base\", triton_inputs)\n",
    "\n",
    "# 추론 결과\n",
    "output = response.as_numpy(\"output\")\n",
    "prediction = np.argmax(output, axis=1)\n",
    "\n",
    "print(\"긍정\" if prediction[0] == 1 else \"부정\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7224caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석할 텍스트를 입력하세요: asd\n",
      "예측된 클래스: 부정, 확률: 85.06%\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# 서버 URL 설정\n",
    "TRITON_SERVER_URL = \"localhost:8000\"\n",
    "tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
    "client = httpclient.InferenceServerClient(url=TRITON_SERVER_URL)\n",
    "\n",
    "# 사용자 입력 받기\n",
    "text = input(\"분석할 텍스트를 입력하세요: \")\n",
    "inputs = tokenizer(text, return_tensors=\"np\", max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "# Triton 서버 추론 요청\n",
    "triton_inputs = [\n",
    "    httpclient.InferInput(\"input_ids\", inputs['input_ids'].shape, \"INT64\"),\n",
    "    httpclient.InferInput(\"attention_mask\", inputs['attention_mask'].shape, \"INT64\"),\n",
    "    httpclient.InferInput(\"token_type_ids\", inputs['token_type_ids'].shape, \"INT64\"),\n",
    "]\n",
    "\n",
    "triton_inputs[0].set_data_from_numpy(inputs['input_ids'])\n",
    "triton_inputs[1].set_data_from_numpy(inputs['attention_mask'])\n",
    "triton_inputs[2].set_data_from_numpy(inputs['token_type_ids'])\n",
    "\n",
    "response = client.infer(\"bert_kor_base\", triton_inputs)\n",
    "output = response.as_numpy(\"output\")\n",
    "\n",
    "# 확률 계산 및 출력\n",
    "probabilities = F.softmax(torch.tensor(output), dim=1).numpy()\n",
    "predicted_class = np.argmax(probabilities, axis=1)\n",
    "confidence_score = probabilities[0, predicted_class[0]] * 100\n",
    "\n",
    "print(f\"예측된 클래스: {'긍정' if predicted_class[0] == 1 else '부정'}, 확률: {confidence_score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perf_analyzer -m your_model_name --shape INPUT_NAME:DIM1,DIM2\n",
    "perf_analyzer -m bert_kor_base --shape \"input_ids:1,128\" --shape \"attention_mask:1,128\" --shape \"token_type_ids:1,128\"\n",
    "sudo docker run --rm --gpus=1 -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /home/eternal/testpy:/models nvcr.io/nvidia/tritonserver:23.10-py3 tritonserver --model-repository=/models\n",
    "sudo docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:23.10-py3-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d7289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.43674731254577637 seconds\n",
      "Throughput: 2289.6534707244205 requests/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "# 추론 서버 URL\n",
    "url = 'http://localhost:8000/v2/models/your_model_name/infer'\n",
    "\n",
    "# 요청 데이터\n",
    "data = {'input': 'your_input_data'}\n",
    "\n",
    "# 요청 반복 횟수\n",
    "num_requests = 1000\n",
    "\n",
    "# 쓰루풋 측정 시작\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(num_requests):\n",
    "    response = requests.post(url, json=data)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 총 소요 시간과 쓰루풋 계산\n",
    "total_time = end_time - start_time\n",
    "throughput = num_requests / total_time\n",
    "\n",
    "print(f\"Total time: {total_time} seconds\")\n",
    "print(f\"Throughput: {throughput} requests/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf3c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d4034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad272261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8251b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "*** Measurement Settings ***\n",
    "  Batch size: 1\n",
    "  Service Kind: Triton\n",
    "  Using \"time_windows\" mode for stabilization\n",
    "  Measurement window: 5000 msec\n",
    "  Using synchronous calls for inference\n",
    "  Stabilizing using average latency\n",
    "\n",
    "Request concurrency: 1\n",
    "  Client: \n",
    "    Request count: 9907\n",
    "    Throughput: 550.257 infer/sec\n",
    "    Avg latency: 1816 usec (standard deviation 120 usec)\n",
    "    p50 latency: 1796 usec\n",
    "    p90 latency: 1881 usec\n",
    "    p95 latency: 1936 usec\n",
    "    p99 latency: 2107 usec\n",
    "    Avg HTTP time: 1815 usec (send/recv 12 usec + response wait 1803 usec)\n",
    "  Server: \n",
    "    Inference count: 9907\n",
    "    Execution count: 9907\n",
    "    Successful request count: 9907\n",
    "    Avg request latency: 1757 usec (overhead 4 usec + queue 4 usec + compute input 7 usec + compute infer 1739 usec + compute output 2 usec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51937e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쓰루풋: 24.09597661400463 요청/초\n"
     ]
    }
   ],
   "source": [
    "import tritonclient.http as httpclient\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Triton 서버 설정\n",
    "triton_url = \"localhost:8000\"  # Triton 서버 URL\n",
    "model_name = \"bert_kor_base\"\n",
    "\n",
    "# 클라이언트 생성\n",
    "client = httpclient.InferenceServerClient(url=triton_url)\n",
    "\n",
    "# 요청을 위한 입력 데이터 생성\n",
    "input_ids = np.random.randint(0, 20000, (1, 128), dtype=np.int64)\n",
    "attention_mask = np.ones((1, 128), dtype=np.int64)\n",
    "token_type_ids = np.zeros((1, 128), dtype=np.int64)\n",
    "\n",
    "# 입력 데이터를 Triton에 맞게 포맷팅\n",
    "inputs = [\n",
    "    httpclient.InferInput(\"input_ids\", [1, 128], \"INT64\"),\n",
    "    httpclient.InferInput(\"attention_mask\", [1, 128], \"INT64\"),\n",
    "    httpclient.InferInput(\"token_type_ids\", [1, 128], \"INT64\")\n",
    "]\n",
    "inputs[0].set_data_from_numpy(input_ids)\n",
    "inputs[1].set_data_from_numpy(attention_mask)\n",
    "inputs[2].set_data_from_numpy(token_type_ids)\n",
    "\n",
    "# 쓰루풋 측정을 위한 시간 측정 시작\n",
    "start_time = time.time()\n",
    "\n",
    "# 요청 반복\n",
    "num_requests = 100\n",
    "for _ in range(num_requests):\n",
    "    response = client.infer(model_name, inputs)\n",
    "\n",
    "# 쓰루풋 계산\n",
    "elapsed_time = time.time() - start_time\n",
    "throughput = num_requests / elapsed_time\n",
    "\n",
    "print(f\"쓰루풋: {throughput} 요청/초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d3f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e485b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cd746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070700c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aacfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "# 추론 서버 URL\n",
    "url = 'http://localhost:8000/v2/models/your_model_name/infer'\n",
    "\n",
    "# 요청 데이터\n",
    "data = {'input': 'your_input_data'}\n",
    "\n",
    "# 요청 반복 횟수\n",
    "num_requests = 1000\n",
    "\n",
    "# 쓰루풋 측정 시작\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(num_requests):\n",
    "    response = requests.post(url, json=data)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 총 소요 시간과 쓰루풋 계산\n",
    "total_time = end_time - start_time\n",
    "throughput = num_requests / total_time\n",
    "\n",
    "print(f\"Total time: {total_time} seconds\")\n",
    "print(f\"Throughput: {throughput} requests/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e1f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658da05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37382253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d677d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223de20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511d4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed15855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbf5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd1f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938437f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0294e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76697a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2828f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441c703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
